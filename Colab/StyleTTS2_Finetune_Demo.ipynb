{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvadigauvce/StyleTTS2/blob/main/Colab/StyleTTS2_Finetune_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install packages and download models"
      ],
      "metadata": {
        "id": "yLqBa4uYPrqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone https://github.com/yl4579/StyleTTS2.git\n",
        "cd StyleTTS2\n",
        "pip install SoundFile torchaudio munch torch pydub pyyaml librosa nltk matplotlib accelerate transformers phonemizer einops einops-exts tqdm typing-extensions git+https://github.com/resemble-ai/monotonic_align.git\n",
        "sudo apt-get install espeak-ng\n",
        "git-lfs clone https://huggingface.co/yl4579/StyleTTS2-LibriTTS\n",
        "mv StyleTTS2-LibriTTS/Models ."
      ],
      "metadata": {
        "id": "H72WF06ZPrTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d400db0-18c8-44e1-b8f7-b771dffd2199"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleTTS2'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Total 372 (delta 0), reused 0 (delta 0), pack-reused 372 (from 1)\u001b[K\n",
            "Receiving objects: 100% (372/372), 133.98 MiB | 14.52 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n",
            "Updating files: 100% (48/48), done.\n",
            "Collecting git+https://github.com/resemble-ai/monotonic_align.git\n",
            "  Cloning https://github.com/resemble-ai/monotonic_align.git to /tmp/pip-req-build-4qezwvjr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/resemble-ai/monotonic_align.git /tmp/pip-req-build-4qezwvjr\n",
            "  Resolved https://github.com/resemble-ai/monotonic_align.git to commit 78b985be210a03d08bc3acc01c4df0442105366f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: SoundFile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting munch\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting phonemizer\n",
            "  Downloading phonemizer-3.3.0-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting einops-exts\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from SoundFile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from SoundFile) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Collecting segments (from phonemizer)\n",
            "  Downloading segments-2.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.11/dist-packages (from phonemizer) (25.1.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Downloading dlinfo-2.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->SoundFile) (2.22)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Downloading csvw-3.5.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.17.0)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.23.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading phonemizer-3.3.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.8/103.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading dlinfo-2.0.0-py3-none-any.whl (3.7 kB)\n",
            "Downloading segments-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading csvw-3.5.1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: monotonic_align\n",
            "  Building wheel for monotonic_align (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for monotonic_align: filename=monotonic_align-1.2-cp311-cp311-linux_x86_64.whl size=1507092 sha256=b375f70ccfbe1a4ebe4c4cd5c02b8277fbd0e79020957ecd27181527a90f6415\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ve69i5hu/wheels/1d/2b/08/d64cc3d7fd672fe9e8bfa2a6f8ddb51b223b2d2b1e4e1a3471\n",
            "Successfully built monotonic_align\n",
            "Installing collected packages: rfc3986, pydub, language-tags, rdflib, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, monotonic_align, isodate, einops-exts, dlinfo, colorama, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, csvw, segments, phonemizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed colorama-0.4.6 csvw-3.5.1 dlinfo-2.0.0 einops-exts-0.0.4 isodate-0.7.2 language-tags-1.2.0 monotonic_align-1.2 munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 phonemizer-3.3.0 pydub-0.25.1 rdflib-7.1.3 rfc3986-1.5.0 segments-2.3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 4,526 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcaudio0 amd64 1.1-6build2 [8,956 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsonic0 amd64 0.2.0-11build1 [10.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 espeak-ng-data amd64 1.50+dfsg-10ubuntu0.1 [3,956 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libespeak-ng1 amd64 1.50+dfsg-10ubuntu0.1 [207 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 espeak-ng amd64 1.50+dfsg-10ubuntu0.1 [343 kB]\n",
            "Fetched 4,526 kB in 2s (1,992 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
            "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'StyleTTS2-LibriTTS'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 13 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (25/25), 3.73 KiB | 764.00 KiB/s, done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset (LJSpeech, 200 samples, ~15 minutes of data)\n",
        "\n",
        "You can definitely do it with fewer samples. This is just a proof of concept with 200 smaples."
      ],
      "metadata": {
        "id": "G398sL8wPzTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd StyleTTS2\n",
        "!rm -rf Data"
      ],
      "metadata": {
        "id": "kJuQUBrEPy5C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbe25a3-30ac-4739-d0cf-b1e0bc61e448"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/StyleTTS2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown --id 1vqz26D3yn7OXS2vbfYxfSnpLS6m6tOFP\n",
        "!gdown --id 1UO_8yjJtPXWIYk6vXa1rOKhrqjpwEqZ8\n",
        "!unzip Data.zip"
      ],
      "metadata": {
        "id": "mDXW8ZZePuSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02db2a3b-89d6-4822-91b8-a7f394689e60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UO_8yjJtPXWIYk6vXa1rOKhrqjpwEqZ8\n",
            "From (redirected): https://drive.google.com/uc?id=1UO_8yjJtPXWIYk6vXa1rOKhrqjpwEqZ8&confirm=t&uuid=04f6ab37-e2ca-47ad-b544-718c46c4358a\n",
            "To: /content/StyleTTS2/Data.zip\n",
            "100% 49.1M/49.1M [00:00<00:00, 65.3MB/s]\n",
            "Archive:  Data.zip\n",
            "   creating: Data/\n",
            "  inflating: Data/LJ001-0048.wav     \n",
            "  inflating: Data/LJ001-0060.wav     \n",
            "  inflating: Data/LJ001-0074.wav     \n",
            "  inflating: Data/LJ001-0128.wav     \n",
            "  inflating: Data/LJ001-0114.wav     \n",
            "  inflating: Data/LJ001-0100.wav     \n",
            "  inflating: Data/LJ001-0101.wav     \n",
            "  inflating: Data/LJ001-0115.wav     \n",
            "  inflating: Data/LJ001-0129.wav     \n",
            "  inflating: Data/LJ001-0075.wav     \n",
            "  inflating: Data/LJ001-0061.wav     \n",
            "  inflating: Data/LJ001-0049.wav     \n",
            "  inflating: Data/LJ001-0077.wav     \n",
            "  inflating: Data/LJ001-0063.wav     \n",
            "  inflating: Data/LJ001-0088.wav     \n",
            "  inflating: Data/LJ001-0103.wav     \n",
            "  inflating: Data/LJ001-0117.wav     \n",
            "  inflating: Data/LJ001-0116.wav     \n",
            "  inflating: Data/LJ001-0102.wav     \n",
            "  inflating: Data/LJ001-0089.wav     \n",
            "  inflating: Data/LJ001-0062.wav     \n",
            "  inflating: Data/LJ001-0076.wav     \n",
            "  inflating: Data/LJ001-0072.wav     \n",
            "  inflating: Data/LJ001-0066.wav     \n",
            "  inflating: Data/LJ001-0099.wav     \n",
            "  inflating: Data/LJ001-0106.wav     \n",
            "  inflating: Data/LJ001-0112.wav     \n",
            "  inflating: Data/LJ001-0113.wav     \n",
            "  inflating: Data/LJ001-0107.wav     \n",
            "  inflating: Data/LJ001-0098.wav     \n",
            "  inflating: Data/LJ001-0067.wav     \n",
            "  inflating: Data/LJ001-0073.wav     \n",
            "  inflating: Data/LJ001-0065.wav     \n",
            "  inflating: Data/LJ001-0071.wav     \n",
            "  inflating: Data/LJ001-0059.wav     \n",
            "  inflating: Data/LJ001-0111.wav     \n",
            "  inflating: Data/LJ001-0105.wav     \n",
            "  inflating: Data/LJ001-0139.wav     \n",
            "  inflating: Data/LJ001-0138.wav     \n",
            "  inflating: Data/LJ001-0104.wav     \n",
            "  inflating: Data/LJ001-0110.wav     \n",
            "  inflating: Data/LJ001-0058.wav     \n",
            "  inflating: Data/LJ001-0070.wav     \n",
            "  inflating: Data/LJ001-0064.wav     \n",
            "  inflating: Data/LJ001-0003.wav     \n",
            "  inflating: Data/LJ001-0017.wav     \n",
            "  inflating: Data/LJ001-0177.wav     \n",
            "  inflating: Data/LJ001-0163.wav     \n",
            "  inflating: Data/LJ001-0162.wav     \n",
            "  inflating: Data/LJ001-0176.wav     \n",
            "  inflating: Data/LJ001-0016.wav     \n",
            "  inflating: Data/LJ001-0002.wav     \n",
            "  inflating: Data/LJ001-0028.wav     \n",
            "  inflating: Data/LJ001-0014.wav     \n",
            "  inflating: Data/LJ001-0148.wav     \n",
            "  inflating: Data/LJ001-0160.wav     \n",
            "  inflating: Data/LJ001-0174.wav     \n",
            "  inflating: Data/LJ001-0175.wav     \n",
            "  inflating: Data/LJ001-0161.wav     \n",
            "  inflating: Data/LJ001-0149.wav     \n",
            "  inflating: Data/LJ001-0001.wav     \n",
            "  inflating: Data/LJ001-0015.wav     \n",
            "  inflating: Data/LJ001-0029.wav     \n",
            "  inflating: Data/LJ001-0011.wav     \n",
            "  inflating: Data/LJ001-0005.wav     \n",
            "  inflating: Data/LJ001-0039.wav     \n",
            "  inflating: Data/LJ001-0165.wav     \n",
            "  inflating: Data/LJ001-0171.wav     \n",
            "  inflating: Data/LJ001-0159.wav     \n",
            "  inflating: Data/LJ001-0158.wav     \n",
            "  inflating: Data/LJ001-0170.wav     \n",
            "  inflating: Data/LJ001-0164.wav     \n",
            "  inflating: Data/LJ001-0038.wav     \n",
            "  inflating: Data/LJ001-0004.wav     \n",
            "  inflating: Data/LJ001-0010.wav     \n",
            "  inflating: Data/LJ001-0006.wav     \n",
            "  inflating: Data/LJ001-0012.wav     \n",
            "  inflating: Data/LJ001-0172.wav     \n",
            "  inflating: Data/LJ001-0166.wav     \n",
            "  inflating: Data/LJ001-0167.wav     \n",
            "  inflating: Data/LJ001-0173.wav     \n",
            "  inflating: Data/LJ001-0013.wav     \n",
            "  inflating: Data/LJ001-0007.wav     \n",
            "  inflating: Data/LJ001-0022.wav     \n",
            "  inflating: Data/LJ001-0036.wav     \n",
            "  inflating: Data/LJ001-0156.wav     \n",
            "  inflating: Data/LJ001-0142.wav     \n",
            "  inflating: Data/LJ001-0181.wav     \n",
            "  inflating: Data/LJ001-0180.wav     \n",
            "  inflating: Data/LJ001-0143.wav     \n",
            "  inflating: Data/LJ001-0157.wav     \n",
            "  inflating: Data/LJ001-0037.wav     \n",
            "  inflating: Data/LJ001-0023.wav     \n",
            "  inflating: Data/LJ001-0009.wav     \n",
            "  inflating: Data/LJ001-0035.wav     \n",
            "  inflating: Data/LJ001-0021.wav     \n",
            "  inflating: Data/LJ001-0169.wav     \n",
            "  inflating: Data/LJ001-0141.wav     \n",
            "  inflating: Data/LJ001-0155.wav     \n",
            "  inflating: Data/LJ001-0182.wav     \n",
            "  inflating: Data/LJ001-0183.wav     \n",
            "  inflating: Data/LJ001-0154.wav     \n",
            "  inflating: Data/LJ001-0140.wav     \n",
            "  inflating: Data/LJ001-0168.wav     \n",
            "  inflating: Data/LJ001-0020.wav     \n",
            "  inflating: Data/LJ001-0034.wav     \n",
            "  inflating: Data/LJ001-0008.wav     \n",
            "  inflating: Data/LJ001-0030.wav     \n",
            "  inflating: Data/LJ001-0024.wav     \n",
            "  inflating: Data/LJ001-0018.wav     \n",
            "  inflating: Data/LJ001-0144.wav     \n",
            "  inflating: Data/LJ001-0150.wav     \n",
            "  inflating: Data/LJ001-0178.wav     \n",
            "  inflating: Data/LJ001-0186.wav     \n",
            "  inflating: Data/LJ001-0179.wav     \n",
            "  inflating: Data/LJ001-0151.wav     \n",
            "  inflating: Data/LJ001-0145.wav     \n",
            "  inflating: Data/LJ001-0019.wav     \n",
            "  inflating: Data/LJ001-0025.wav     \n",
            "  inflating: Data/LJ001-0031.wav     \n",
            "  inflating: Data/LJ001-0027.wav     \n",
            "  inflating: Data/LJ001-0033.wav     \n",
            "  inflating: Data/LJ001-0153.wav     \n",
            "  inflating: Data/LJ001-0147.wav     \n",
            "  inflating: Data/LJ001-0184.wav     \n",
            "  inflating: Data/LJ001-0185.wav     \n",
            "  inflating: Data/LJ001-0146.wav     \n",
            "  inflating: Data/LJ001-0152.wav     \n",
            "  inflating: Data/LJ001-0032.wav     \n",
            "  inflating: Data/LJ001-0026.wav     \n",
            "  inflating: Data/LJ001-0069.wav     \n",
            "  inflating: Data/LJ001-0041.wav     \n",
            "  inflating: Data/LJ001-0055.wav     \n",
            "  inflating: Data/LJ001-0082.wav     \n",
            "  inflating: Data/LJ001-0096.wav     \n",
            "  inflating: Data/LJ001-0109.wav     \n",
            "  inflating: Data/LJ001-0135.wav     \n",
            "  inflating: Data/LJ001-0121.wav     \n",
            "  inflating: Data/LJ001-0120.wav     \n",
            "  inflating: Data/LJ001-0134.wav     \n",
            "  inflating: Data/LJ001-0108.wav     \n",
            "  inflating: Data/LJ001-0097.wav     \n",
            "  inflating: Data/LJ001-0083.wav     \n",
            "  inflating: Data/LJ001-0054.wav     \n",
            "  inflating: Data/LJ001-0040.wav     \n",
            "  inflating: Data/LJ001-0068.wav     \n",
            "  inflating: Data/LJ001-0056.wav     \n",
            "  inflating: Data/LJ001-0042.wav     \n",
            "  inflating: Data/LJ001-0095.wav     \n",
            "  inflating: Data/LJ001-0081.wav     \n",
            "  inflating: Data/LJ001-0122.wav     \n",
            "  inflating: Data/LJ001-0136.wav     \n",
            "  inflating: Data/LJ001-0137.wav     \n",
            "  inflating: Data/LJ001-0123.wav     \n",
            "  inflating: Data/LJ001-0080.wav     \n",
            "  inflating: Data/LJ001-0094.wav     \n",
            "  inflating: Data/LJ001-0043.wav     \n",
            "  inflating: Data/LJ001-0057.wav     \n",
            "  inflating: Data/LJ001-0053.wav     \n",
            "  inflating: Data/LJ001-0047.wav     \n",
            "  inflating: Data/LJ001-0090.wav     \n",
            "  inflating: Data/LJ001-0084.wav     \n",
            "  inflating: Data/LJ001-0127.wav     \n",
            "  inflating: Data/LJ001-0133.wav     \n",
            "  inflating: Data/LJ001-0132.wav     \n",
            "  inflating: Data/LJ001-0126.wav     \n",
            "  inflating: Data/LJ001-0085.wav     \n",
            "  inflating: Data/LJ001-0091.wav     \n",
            "  inflating: Data/LJ001-0046.wav     \n",
            "  inflating: Data/LJ001-0052.wav     \n",
            "  inflating: Data/LJ001-0044.wav     \n",
            "  inflating: Data/LJ001-0050.wav     \n",
            "  inflating: Data/LJ001-0078.wav     \n",
            "  inflating: Data/LJ001-0087.wav     \n",
            "  inflating: Data/LJ001-0093.wav     \n",
            "  inflating: Data/LJ001-0130.wav     \n",
            "  inflating: Data/LJ001-0124.wav     \n",
            "  inflating: Data/LJ001-0118.wav     \n",
            "  inflating: Data/LJ001-0119.wav     \n",
            "  inflating: Data/LJ001-0125.wav     \n",
            "  inflating: Data/LJ001-0131.wav     \n",
            "  inflating: Data/LJ001-0092.wav     \n",
            "  inflating: Data/LJ001-0086.wav     \n",
            "  inflating: Data/LJ001-0079.wav     \n",
            "  inflating: Data/LJ001-0051.wav     \n",
            "  inflating: Data/LJ001-0045.wav     \n",
            "  inflating: 200_sample.txt          \n",
            "  inflating: __MACOSX/._200_sample.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_name = \"200_sample.txt\"\n",
        "output_file_name = \"phonemized_200.txt\"\n",
        "# load phonemizer\n",
        "import phonemizer\n",
        "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
        "\n",
        "def text_to_phonemes(text):\n",
        "    text = text.strip()\n",
        "    ps = global_phonemizer.phonemize([text])\n",
        "    return ps[0]\n",
        "\n",
        "file_out = open(output_file_name, 'w')\n",
        "\n",
        "with open(input_file_name, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      #print(line)\n",
        "      wave_file, text = line.split('|', 1)\n",
        "      wave_file_name = wave_file + '.wav'\n",
        "      #print(wave_file_name, text)\n",
        "      phonemized = text_to_phonemes(text)\n",
        "      #print(phonemized)\n",
        "      #file_out.write(wave_file_name+ '.wav' + '|' + phonemized +'|'+'0'+'\\n')\n",
        "      file_out.write(f\"{wave_file_name}|{phonemized}|{0}\\n\")\n",
        "file_out.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_el2nv5ons8",
        "outputId": "e61829ed-73a8-4528-d92f-21ef4988d1c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 600.0% of the lines (6/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 300.0% of the lines (3/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 400.0% of the lines (4/1)\n",
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n",
            "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 150 phonemized_200.txt > Data/train_list.txt\n",
        "!tail -n 50 phonemized_200.txt > Data/val_list.txt\n",
        "!head -n 150 phonemized_200.txt > Data/OOD_texts.txt"
      ],
      "metadata": {
        "id": "BTULOqS6t-89"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change the finetuning config\n",
        "\n",
        "Depending on the GPU you got, you may want to change the bacth size, max audio length, epiochs and so on."
      ],
      "metadata": {
        "id": "_AlBQREWU8ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"Configs/config_ft.yml\"\n",
        "\n",
        "import yaml\n",
        "config = yaml.safe_load(open(config_path))\n",
        "!cat Configs/config_ft.yml\n",
        "!cat Data/train_list.txt"
      ],
      "metadata": {
        "id": "7uEITi0hU4I2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68edf04-b445-426f-d634-a980d4fd372f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_dir: \"Models/LJSpeech\"\n",
            "save_freq: 5\n",
            "log_interval: 10\n",
            "device: \"cuda\"\n",
            "epochs: 50 # number of finetuning epoch (1 hour of data)\n",
            "batch_size: 8\n",
            "max_len: 400 # maximum number of frames\n",
            "pretrained_model: \"Models/LibriTTS/epochs_2nd_00020.pth\"\n",
            "second_stage_load_pretrained: true # set to true if the pre-trained model is for 2nd stage\n",
            "load_only_params: true # set to true if do not want to load epoch numbers and optimizer parameters\n",
            "\n",
            "F0_path: \"Utils/JDC/bst.t7\"\n",
            "ASR_config: \"Utils/ASR/config.yml\"\n",
            "ASR_path: \"Utils/ASR/epoch_00080.pth\"\n",
            "PLBERT_dir: 'Utils/PLBERT/'\n",
            "\n",
            "data_params:\n",
            "  train_data: \"Data/train_list.txt\"\n",
            "  val_data: \"Data/val_list.txt\"\n",
            "  root_path: \"/local/LJSpeech-1.1/wavs\"\n",
            "  OOD_data: \"Data/OOD_texts.txt\"\n",
            "  min_length: 50 # sample until texts with this size are obtained for OOD texts\n",
            "\n",
            "preprocess_params:\n",
            "  sr: 24000\n",
            "  spect_params:\n",
            "    n_fft: 2048\n",
            "    win_length: 1200\n",
            "    hop_length: 300\n",
            "\n",
            "model_params:\n",
            "  multispeaker: true\n",
            "\n",
            "  dim_in: 64 \n",
            "  hidden_dim: 512\n",
            "  max_conv_dim: 512\n",
            "  n_layer: 3\n",
            "  n_mels: 80\n",
            "\n",
            "  n_token: 178 # number of phoneme tokens\n",
            "  max_dur: 50 # maximum duration of a single phoneme\n",
            "  style_dim: 128 # style vector size\n",
            "  \n",
            "  dropout: 0.2\n",
            "\n",
            "  # config for decoder\n",
            "  decoder: \n",
            "      type: 'hifigan' # either hifigan or istftnet\n",
            "      resblock_kernel_sizes: [3,7,11]\n",
            "      upsample_rates :  [10,5,3,2]\n",
            "      upsample_initial_channel: 512\n",
            "      resblock_dilation_sizes: [[1,3,5], [1,3,5], [1,3,5]]\n",
            "      upsample_kernel_sizes: [20,10,6,4]\n",
            "      \n",
            "  # speech language model config\n",
            "  slm:\n",
            "      model: 'microsoft/wavlm-base-plus'\n",
            "      sr: 16000 # sampling rate of SLM\n",
            "      hidden: 768 # hidden size of SLM\n",
            "      nlayers: 13 # number of layers of SLM\n",
            "      initial_channel: 64 # initial channels of SLM discriminator head\n",
            "  \n",
            "  # style diffusion model config\n",
            "  diffusion:\n",
            "    embedding_mask_proba: 0.1\n",
            "    # transformer config\n",
            "    transformer:\n",
            "      num_layers: 3\n",
            "      num_heads: 8\n",
            "      head_features: 64\n",
            "      multiplier: 2\n",
            "\n",
            "    # diffusion distribution config\n",
            "    dist:\n",
            "      sigma_data: 0.2 # placeholder for estimate_sigma_data set to false\n",
            "      estimate_sigma_data: true # estimate sigma_data from the current batch if set to true\n",
            "      mean: -3.0\n",
            "      std: 1.0\n",
            "  \n",
            "loss_params:\n",
            "    lambda_mel: 5. # mel reconstruction loss\n",
            "    lambda_gen: 1. # generator loss\n",
            "    lambda_slm: 1. # slm feature matching loss\n",
            "    \n",
            "    lambda_mono: 1. # monotonic alignment loss (TMA)\n",
            "    lambda_s2s: 1. # sequence-to-sequence loss (TMA)\n",
            "\n",
            "    lambda_F0: 1. # F0 reconstruction loss\n",
            "    lambda_norm: 1. # norm reconstruction loss\n",
            "    lambda_dur: 1. # duration loss\n",
            "    lambda_ce: 20. # duration predictor probability output CE loss\n",
            "    lambda_sty: 1. # style reconstruction loss\n",
            "    lambda_diff: 1. # score matching loss\n",
            "    \n",
            "    diff_epoch: 10 # style diffusion starting epoch\n",
            "    joint_epoch: 30 # joint training starting epoch\n",
            "\n",
            "optimizer_params:\n",
            "  lr: 0.0001 # general learning rate\n",
            "  bert_lr: 0.00001 # learning rate for PLBERT\n",
            "  ft_lr: 0.0001 # learning rate for acoustic modules\n",
            "  \n",
            "slmadv_params:\n",
            "  min_len: 400 # minimum length of samples\n",
            "  max_len: 500 # maximum length of samples\n",
            "  batch_percentage: 0.5 # to prevent out of memory, only use half of the original batch size\n",
            "  iter: 10 # update the discriminator every this iterations of generator update\n",
            "  thresh: 5 # gradient norm above which the gradient is scaled\n",
            "  scale: 0.01 # gradient scaling factor for predictors from SLM discriminators\n",
            "  sig: 1.5 # sigma for differentiable duration modeling\n",
            "  \n",
            "LJ001-0001.wav|pɹˈɪntɪŋ, ɪnðɪ ˈoʊnli sˈɛns wɪð wˌɪtʃ wiː ɑːɹ æt pɹˈɛzənt kənsˈɜːnd, dˈɪfɚz fɹʌm mˈoʊst ɪf nˌɑːt fɹʌm ˈɔːl ðɪ ˈɑːɹts ænd kɹˈæfts ɹˌɛpɹᵻzˈɛntᵻd ɪnðɪ ɛksɪbˈɪʃən pɹˈɪntɪŋ, ɪnðɪ ˈoʊnli sˈɛns wɪð wˌɪtʃ wiː ɑːɹ æt pɹˈɛzənt kənsˈɜːnd, dˈɪfɚz fɹʌm mˈoʊst ɪf nˌɑːt fɹʌm ˈɔːl ðɪ ˈɑːɹts ænd kɹˈæfts ɹˌɛpɹᵻzˈɛntᵻd ɪnðɪ ɛksɪbˈɪʃən |0\n",
            "LJ001-0002.wav|ɪn bˌiːɪŋ kəmpˈæɹətˌɪvli mˈɑːdɚn.ɪn bˌiːɪŋ kəmpˈæɹətˌɪvli mˈɑːdɚn. |0\n",
            "LJ001-0003.wav|fɔːɹ ɔːlðˈoʊ ðə tʃaɪnˈiːz tˈʊk ɪmpɹˈɛʃənz fɹʌm wˈʊd blˈɑːks ɛŋɡɹˈeɪvd ɪn ɹᵻlˈiːf fɔːɹ sˈɛntʃɚɹiz bᵻfˌoːɹ ðə wˈʊdkʌɾɚz ʌvðə nˈɛðɜːləndz, baɪ ɐ sˈɪmɪlɚ pɹˈɑːsɛs fɔːɹ ɔːlðˈoʊ ðə tʃaɪnˈiːz tˈʊk ɪmpɹˈɛʃənz fɹʌm wˈʊd blˈɑːks ɛŋɡɹˈeɪvd ɪn ɹᵻlˈiːf fɔːɹ sˈɛntʃɚɹiz bᵻfˌoːɹ ðə wˈʊdkʌɾɚz ʌvðə nˈɛðɜːləndz, baɪ ɐ sˈɪmɪlɚ pɹˈɑːsɛs |0\n",
            "LJ001-0004.wav|pɹədˈuːst ðə blˈɑːk bˈʊks, wˌɪtʃ wɜː ðɪ ɪmˈiːdɪət pɹˈɛdᵻsˌɛsɚz ʌvðə tɹˈuː pɹˈɪntᵻd bˈʊk,pɹədˈuːst ðə blˈɑːk bˈʊks, wˌɪtʃ wɜː ðɪ ɪmˈiːdɪət pɹˈɛdᵻsˌɛsɚz ʌvðə tɹˈuː pɹˈɪntᵻd bˈʊk, |0\n",
            "LJ001-0005.wav|ðɪ ɪnvˈɛnʃən ʌv mˈuːvəbəl mˈɛɾəl lˈɛɾɚz ɪnðə mˈɪdəl ʌvðə fˈɪftiːnθ sˈɛntʃɚɹi mˈeɪ dʒˈʌstli biː kənsˈɪdɚd æz ðɪ ɪnvˈɛnʃən ʌvðɪ ˈɑːɹt ʌv pɹˈɪntɪŋ.ðɪ ɪnvˈɛnʃən ʌv mˈuːvəbəl mˈɛɾəl lˈɛɾɚz ɪnðə mˈɪdəl ʌvðə fˈɪftiːnθ sˈɛntʃɚɹi mˈeɪ dʒˈʌstli biː kənsˈɪdɚd æz ðɪ ɪnvˈɛnʃən ʌvðɪ ˈɑːɹt ʌv pɹˈɪntɪŋ. |0\n",
            "LJ001-0006.wav|ænd ɪɾ ɪz wˈɜːθ mˈɛnʃən ɪn pˈæsɪŋ ðˈæt, æz ɐn ɛɡzˈæmpəl ʌv fˈaɪn taɪpˈɑːɡɹəfi,ænd ɪɾ ɪz wˈɜːθ mˈɛnʃən ɪn pˈæsɪŋ ðˈæt, æz ɐn ɛɡzˈæmpəl ʌv fˈaɪn taɪpˈɑːɡɹəfi, |0\n",
            "LJ001-0007.wav|ðɪ ˈɜːlɪɪst bˈʊk pɹˈɪntᵻd wɪð mˈuːvəbəl tˈaɪps, ðə ɡjˈuːtənbˌɜːɡ, ɔːɹ \"fˈɔːɹɾitˈuː lˈaɪn bˈaɪbəl\" ʌv ɐbˌaʊt wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd fˈɪfti fˈaɪv,ðɪ ˈɜːlɪɪst bˈʊk pɹˈɪntᵻd wɪð mˈuːvəbəl tˈaɪps, ðə ɡjˈuːtənbˌɜːɡ, ɔːɹ \"fˈɔːɹɾitˈuː lˈaɪn bˈaɪbəl\" ʌv ɐbˌaʊt fˈoːɹtiːn fˈɪftifˈaɪv, |0\n",
            "LJ001-0008.wav|hɐz nˈɛvɚ bˌɪn sɚpˈæst.hɐz nˈɛvɚ bˌɪn sɚpˈæst. |0\n",
            "LJ001-0009.wav|pɹˈɪntɪŋ, ðˈɛn, fɔːɹ ˌaʊɚ pˈɜːpəs, mˈeɪ biː kənsˈɪdɚd æz ðɪ ˈɑːɹt ʌv mˌeɪkɪŋ bˈʊks baɪ mˈiːnz ʌv mˈuːvəbəl tˈaɪps.pɹˈɪntɪŋ, ðˈɛn, fɔːɹ ˌaʊɚ pˈɜːpəs, mˈeɪ biː kənsˈɪdɚd æz ðɪ ˈɑːɹt ʌv mˌeɪkɪŋ bˈʊks baɪ mˈiːnz ʌv mˈuːvəbəl tˈaɪps. |0\n",
            "LJ001-0010.wav|nˈaʊ, æz ˈɔːl bˈʊks nˌɑːt pɹaɪmˈɛɹᵻli ɪntˈɛndᵻd æz pˈɪktʃɚbˈʊks kənsˈɪst pɹˈɪnsɪpəli ʌv tˈaɪps kəmpˈoʊzd tə fˈɔːɹm lˈɛɾɚpɹˌɛs,nˈaʊ, æz ˈɔːl bˈʊks nˌɑːt pɹaɪmˈɛɹᵻli ɪntˈɛndᵻd æz pˈɪktʃɚbˈʊks kənsˈɪst pɹˈɪnsɪpəli ʌv tˈaɪps kəmpˈoʊzd tə fˈɔːɹm lˈɛɾɚpɹˌɛs, |0\n",
            "LJ001-0011.wav|ɪɾ ɪz ʌvðə fˈɜːst ɪmpˈoːɹtəns ðætðə lˈɛɾɚ jˈuːzd ʃˌʊd biː fˈaɪn ɪn fˈɔːɹm;ɪɾ ɪz ʌvðə fˈɜːst ɪmpˈoːɹtəns ðætðə lˈɛɾɚ jˈuːzd ʃˌʊd biː fˈaɪn ɪn fˈɔːɹm; |0\n",
            "LJ001-0012.wav|ɪspˈɛʃəli æz nˈoʊmˌoːɹ tˈaɪm ɪz ˈɑːkjʊpˌaɪd, ɔːɹ kˈɔst ɪŋkˈɜːd, ɪn kˈæstɪŋ, sˈɛɾɪŋ, ɔːɹ pɹˈɪntɪŋ bjˈuːɾifəl lˈɛɾɚz ɪspˈɛʃəli æz nˈoʊmˌoːɹ tˈaɪm ɪz ˈɑːkjʊpˌaɪd, ɔːɹ kˈɔst ɪŋkˈɜːd, ɪn kˈæstɪŋ, sˈɛɾɪŋ, ɔːɹ pɹˈɪntɪŋ bjˈuːɾifəl lˈɛɾɚz |0\n",
            "LJ001-0013.wav|ðɐn ɪnðə sˈeɪm ˌɑːpɚɹˈeɪʃənz wɪð ˈʌɡli wˌʌnz.ðɐn ɪnðə sˈeɪm ˌɑːpɚɹˈeɪʃənz wɪð ˈʌɡli wˌʌnz. |0\n",
            "LJ001-0014.wav|ænd ɪt wʌzɐ mˈæɾɚɹ ʌv kˈoːɹs ðæt ɪnðə mˈɪdəl ˈeɪdʒᵻz, wˌɛn ðə kɹˈæftsmɛn tˈʊk kˈɛɹ ðæt bjˈuːɾifəl fˈɔːɹm ʃˌʊd ˈɔːlweɪz biː ɐ pˈɑːɹt ʌv ðɛɹ pɹədˈʌkʃənz wʌtˈɛvɚ ðeɪ wɜː,ænd ɪt wʌzɐ mˈæɾɚɹ ʌv kˈoːɹs ðæt ɪnðə mˈɪdəl ˈeɪdʒᵻz, wˌɛn ðə kɹˈæftsmɛn tˈʊk kˈɛɹ ðæt bjˈuːɾifəl fˈɔːɹm ʃˌʊd ˈɔːlweɪz biː ɐ pˈɑːɹt ʌv ðɛɹ pɹədˈʌkʃənz wʌtˈɛvɚ ðeɪ wɜː, |0\n",
            "LJ001-0015.wav|ðə fˈɔːɹmz ʌv pɹˈɪntᵻd lˈɛɾɚz ʃˌʊd biː bjˈuːɾifəl, ænd ðæt ðɛɹ ɚɹˈeɪndʒmənt ɔnðə pˈeɪdʒ ʃˌʊd biː ɹˈiːzənəbəl ænd ɐ hˈɛlp tə ðə ʃˈeɪplinəs ʌvðə lˈɛɾɚz ðɛmsˈɛlvz.ðə fˈɔːɹmz ʌv pɹˈɪntᵻd lˈɛɾɚz ʃˌʊd biː bjˈuːɾifəl, ænd ðæt ðɛɹ ɚɹˈeɪndʒmənt ɔnðə pˈeɪdʒ ʃˌʊd biː ɹˈiːzənəbəl ænd ɐ hˈɛlp tə ðə ʃˈeɪplinəs ʌvðə lˈɛɾɚz ðɛmsˈɛlvz. |0\n",
            "LJ001-0016.wav|ðə mˈɪdəl ˈeɪdʒᵻz bɹˈɔːt kælˈɪɡɹəfi tə pɚfˈɛkʃən, ænd ɪt wʌz nˈætʃɚɹəl ðˈɛɹfoːɹ ðə mˈɪdəl ˈeɪdʒᵻz bɹˈɔːt kælˈɪɡɹəfi tə pɚfˈɛkʃən, ænd ɪt wʌz nˈætʃɚɹəl ðˈɛɹfoːɹ |0\n",
            "LJ001-0017.wav|ðætðə fˈɔːɹmz ʌv pɹˈɪntᵻd lˈɛɾɚz ʃˌʊd fˈɑːloʊ mˈoːɹ ɔːɹ lˈɛs klˈoʊsli ðoʊz ʌvðə ɹˈɪʔn̩ kˈæɹɪktɚ, ænd ðeɪ fˈɑːloʊd ðˌɛm vˈɛɹi klˈoʊsli.ðætðə fˈɔːɹmz ʌv pɹˈɪntᵻd lˈɛɾɚz ʃˌʊd fˈɑːloʊ mˈoːɹ ɔːɹ lˈɛs klˈoʊsli ðoʊz ʌvðə ɹˈɪʔn̩ kˈæɹɪktɚ, ænd ðeɪ fˈɑːloʊd ðˌɛm vˈɛɹi klˈoʊsli. |0\n",
            "LJ001-0018.wav|ðə fˈɜːst bˈʊks wɜː pɹˈɪntᵻd ɪn blˈæk lˈɛɾɚ, ˈaɪ.ˈiː. ðə lˈɛɾɚ wˌɪtʃ wʌzɐ ɡˈɑːθɪk dɪvˈɛləpmənt ʌvðɪ ˈeɪntʃənt ɹˈoʊmən kˈæɹɪktɚ,ðə fˈɜːst bˈʊks wɜː pɹˈɪntᵻd ɪn blˈæk lˈɛɾɚ, ˈaɪ.ˈiː. ðə lˈɛɾɚ wˌɪtʃ wʌzɐ ɡˈɑːθɪk dɪvˈɛləpmənt ʌvðɪ ˈeɪntʃənt ɹˈoʊmən kˈæɹɪktɚ, |0\n",
            "LJ001-0019.wav|ænd wˌɪtʃ dɪvˈɛləpt mˈoːɹ kəmplˈiːtli ænd sˌæɾɪsfˈæktɚɹəli ɔnðə sˈaɪd ʌvðə \"lˈoʊɚkˈeɪs\" ðɐn ðə kˈæpɪɾəl lˈɛɾɚz;ænd wˌɪtʃ dɪvˈɛləpt mˈoːɹ kəmplˈiːtli ænd sˌæɾɪsfˈæktɚɹəli ɔnðə sˈaɪd ʌvðə \"lˈoʊɚkˈeɪs\" ðɐn ðə kˈæpɪɾəl lˈɛɾɚz; |0\n",
            "LJ001-0020.wav|ðə \"lˈoʊɚkˈeɪs\" bˌiːɪŋ ɪn fˈækt ɪnvˈɛntᵻd ɪnðɪ ˈɜːli mˈɪdəl ˈeɪdʒᵻz.ðə \"lˈoʊɚkˈeɪs\" bˌiːɪŋ ɪn fˈækt ɪnvˈɛntᵻd ɪnðɪ ˈɜːli mˈɪdəl ˈeɪdʒᵻz. |0\n",
            "LJ001-0021.wav|ðɪ ˈɜːlɪɪst bˈʊk pɹˈɪntᵻd wɪð mˈuːvəbəl tˈaɪp, ðɪ ɐfˈoːɹsɛd ɡjˈuːtənbˌɜːɡ bˈaɪbəl, ɪz pɹˈɪntᵻd ɪn lˈɛɾɚz wˌɪtʃ ɑːɹ ɐn ɛɡzˈækt ˌɪmɪtˈeɪʃən ðɪ ˈɜːlɪɪst bˈʊk pɹˈɪntᵻd wɪð mˈuːvəbəl tˈaɪp, ðɪ ɐfˈoːɹsɛd ɡjˈuːtənbˌɜːɡ bˈaɪbəl, ɪz pɹˈɪntᵻd ɪn lˈɛɾɚz wˌɪtʃ ɑːɹ ɐn ɛɡzˈækt ˌɪmɪtˈeɪʃən |0\n",
            "LJ001-0022.wav|ʌvðə mˈoːɹ fˈɔːɹməl ᵻklˌiːzɪˈæstɪkəl ɹˈaɪɾɪŋ wˌɪtʃ əbtˈeɪnd æt ðæt tˈaɪm; ðɪs hɐz sˈɪns bˌɪn kˈɔːld \"mˈɪsəl tˈaɪp,\"ʌvðə mˈoːɹ fˈɔːɹməl ᵻklˌiːzɪˈæstɪkəl ɹˈaɪɾɪŋ wˌɪtʃ əbtˈeɪnd æt ðæt tˈaɪm; ðɪs hɐz sˈɪns bˌɪn kˈɔːld \"mˈɪsəl tˈaɪp,\" |0\n",
            "LJ001-0023.wav|ænd wʌz ɪn fˈækt ðə kˈaɪnd ʌv lˈɛɾɚ jˈuːzd ɪnðə mˈɛni splˈɛndɪd mˈɪsəlz, sˈɔltɚz, ɛtsˈɛtɹə., pɹədˈuːst baɪ pɹˈɪntɪŋ ɪnðə fˈɪftiːnθ sˈɛntʃɚɹi.ænd wʌz ɪn fˈækt ðə kˈaɪnd ʌv lˈɛɾɚ jˈuːzd ɪnðə mˈɛni splˈɛndɪd mˈɪsəlz, sˈɔltɚz, ɛtsˈɛtɹə., pɹədˈuːst baɪ pɹˈɪntɪŋ ɪnðə fˈɪftiːnθ sˈɛntʃɚɹi. |0\n",
            "LJ001-0024.wav|bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti tˈuː)bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ fˈoːɹtiːn sˈɪkstitˈuː) |0\n",
            "LJ001-0025.wav|ˈɪmᵻtˌeɪts ɐ mˈʌtʃ fɹˈiːɚ hˈænd, sˈɪmplɚ, ɹˈaʊndɚ, ænd lˈɛs spˈaɪki, ænd ðˈɛɹfoːɹ fˈɑːɹ plˈɛzæntɚ ænd ˈiːziɚ tə ɹˈiːd.ˈɪmᵻtˌeɪts ɐ mˈʌtʃ fɹˈiːɚ hˈænd, sˈɪmplɚ, ɹˈaʊndɚ, ænd lˈɛs spˈaɪki, ænd ðˈɛɹfoːɹ fˈɑːɹ plˈɛzæntɚ ænd ˈiːziɚ tə ɹˈiːd. |0\n",
            "LJ001-0026.wav|ɔnðə hˈoʊl ðə tˈaɪp ʌv ðɪs bˈʊk mˈeɪ biː kənsˈɪdɚd ðə nˈiːplˈʌsˈʌltɹə ʌv ɡˈɑːθɪk tˈaɪp,ɔnðə hˈoʊl ðə tˈaɪp ʌv ðɪs bˈʊk mˈeɪ biː kənsˈɪdɚd ðə nˈiːplˈʌsˈʌltɹə ʌv ɡˈɑːθɪk tˈaɪp, |0\n",
            "LJ001-0027.wav|ɪspˈɛʃəli æz ɹᵻɡˈɑːɹdz ðə lˈoʊɚkˈeɪs lˈɛɾɚz; ænd tˈaɪp vˈɛɹi sˈɪmɪlɚ wʌz jˈuːzd dˈʊɹɹɪŋ ðə nˈɛkst fˈɪftiːn ɔːɹ twˈɛnti jˈɪɹz nˌɑːt ˈoʊnli baɪ skˈoʊfɚ,ɪspˈɛʃəli æz ɹᵻɡˈɑːɹdz ðə lˈoʊɚkˈeɪs lˈɛɾɚz; ænd tˈaɪp vˈɛɹi sˈɪmɪlɚ wʌz jˈuːzd dˈʊɹɹɪŋ ðə nˈɛkst fˈɪftiːn ɔːɹ twˈɛnti jˈɪɹz nˌɑːt ˈoʊnli baɪ skˈoʊfɚ, |0\n",
            "LJ001-0028.wav|bˌʌt baɪ pɹˈɪntɚz ɪn stɹˈæsbɜːɡ, bˈæsəl, pˈæɹɪs, lˈuːbɛk, ænd ˈʌðɚ sˈɪɾiz.bˌʌt baɪ pɹˈɪntɚz ɪn stɹˈæsbɜːɡ, bˈæsəl, pˈæɹɪs, lˈuːbɛk, ænd ˈʌðɚ sˈɪɾiz. |0\n",
            "LJ001-0029.wav|bˌʌt ðˌoʊ ɔnðə hˈoʊl, ɛksˈɛpt ɪn ˈɪɾəli, ɡˈɑːθɪk lˈɛɾɚ wʌz mˈoʊst ˈɔfən jˈuːzd bˌʌt ðˌoʊ ɔnðə hˈoʊl, ɛksˈɛpt ɪn ˈɪɾəli, ɡˈɑːθɪk lˈɛɾɚ wʌz mˈoʊst ˈɔfən jˈuːzd |0\n",
            "LJ001-0030.wav|ɐ vˈɛɹi fjˈuː jˈɪɹz sˈɔː ðə bˈɜːθ ʌv ɹˈoʊmən kˈæɹɪktɚ nˌɑːt ˈoʊnli ɪn ˈɪɾəli, bˌʌt ɪn dʒˈɜːməni ænd fɹˈæns.ɐ vˈɛɹi fjˈuː jˈɪɹz sˈɔː ðə bˈɜːθ ʌv ɹˈoʊmən kˈæɹɪktɚ nˌɑːt ˈoʊnli ɪn ˈɪɾəli, bˌʌt ɪn dʒˈɜːməni ænd fɹˈæns. |0\n",
            "LJ001-0031.wav|ɪn wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti fˈaɪv swˈeɪnhaɪm ænd pˈænɑːɹts bɪɡˈæn pɹˈɪntɪŋ ɪnðə mˈɑːnɐstɚɹi ʌv sˌʌbɪˈɑːkoʊ nˌɪɹ ɹˈoʊm,ɪn fˈoːɹtiːn sˈɪkstifˈaɪv swˈeɪnhaɪm ænd pˈænɑːɹts bɪɡˈæn pɹˈɪntɪŋ ɪnðə mˈɑːnɐstɚɹi ʌv sˌʌbɪˈɑːkoʊ nˌɪɹ ɹˈoʊm, |0\n",
            "LJ001-0032.wav|ænd jˈuːzd ɐn ɛksˈiːdɪŋli bjˈuːɾifəl tˈaɪp, wˌɪtʃ ɪz ˌɪndˈiːd tə lˈʊk æɾə tɹænsˈɪʃən bᵻtwˌiːn ɡˈɑːθɪk ænd ɹˈoʊmən,ænd jˈuːzd ɐn ɛksˈiːdɪŋli bjˈuːɾifəl tˈaɪp, wˌɪtʃ ɪz ˌɪndˈiːd tə lˈʊk æɾə tɹænsˈɪʃən bᵻtwˌiːn ɡˈɑːθɪk ænd ɹˈoʊmən, |0\n",
            "LJ001-0033.wav|bˌʌt wˌɪtʃ mˈʌst sˈɜːʔn̩li hæv kˈʌm fɹʌmðə stˈʌdi ʌvðə twˈɛlfθ ɔːɹ ˈiːvən ðɪ ᵻlˈɛvənθ sˈɛntʃɚɹi ˌɛmˌɛsˈɛs.bˌʌt wˌɪtʃ mˈʌst sˈɜːʔn̩li hæv kˈʌm fɹʌmðə stˈʌdi ʌvðə twˈɛlfθ ɔːɹ ˈiːvən ðɪ ᵻlˈɛvənθ sˈɛntʃɚɹi ˌɛmˌɛsˈɛs. |0\n",
            "LJ001-0034.wav|ðeɪ pɹˈɪntᵻd vˈɛɹi fjˈuː bˈʊks ɪn ðɪs tˈaɪp, θɹˈiː ˈoʊnli; bˌʌt ɪn ðɛɹ vˈɛɹi fˈɜːst bˈʊks ɪn ɹˈoʊm, bɪɡˈɪnɪŋ wɪððə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti ˈeɪt,ðeɪ pɹˈɪntᵻd vˈɛɹi fjˈuː bˈʊks ɪn ðɪs tˈaɪp, θɹˈiː ˈoʊnli; bˌʌt ɪn ðɛɹ vˈɛɹi fˈɜːst bˈʊks ɪn ɹˈoʊm, bɪɡˈɪnɪŋ wɪððə jˈɪɹ fˈoːɹtiːn sˈɪkstiˈeɪt, |0\n",
            "LJ001-0035.wav|ðeɪ dɪskˈɑːɹdᵻd ðɪs fɚɹə mˈoːɹ kəmplˈiːtli ɹˈoʊmən ænd fˈɑːɹ lˈɛs bjˈuːɾifəl lˈɛɾɚ.ðeɪ dɪskˈɑːɹdᵻd ðɪs fɚɹə mˈoːɹ kəmplˈiːtli ɹˈoʊmən ænd fˈɑːɹ lˈɛs bjˈuːɾifəl lˈɛɾɚ. |0\n",
            "LJ001-0036.wav|bˌʌt ɐbˌaʊt ðə sˈeɪm jˈɪɹ mˈɛntɪlˌɪn æt stɹˈæsbɜːɡ bɪɡˈæn tə pɹˈɪnt ɪn ɐ tˈaɪp wˌɪtʃ ɪz dɪstˈɪŋktli ɹˈoʊmən;bˌʌt ɐbˌaʊt ðə sˈeɪm jˈɪɹ mˈɛntɪlˌɪn æt stɹˈæsbɜːɡ bɪɡˈæn tə pɹˈɪnt ɪn ɐ tˈaɪp wˌɪtʃ ɪz dɪstˈɪŋktli ɹˈoʊmən; |0\n",
            "LJ001-0037.wav|ænd ðə nˈɛkst jˈɪɹ ɡˈʌnθɚ zˈaɪnɚɹ æɾ ˈɔːɡsbɜːɡ fˈɑːloʊd sˈuːt;ænd ðə nˈɛkst jˈɪɹ ɡˈʌnθɚ zˈaɪnɚɹ æɾ ˈɔːɡsbɜːɡ fˈɑːloʊd sˈuːt; |0\n",
            "LJ001-0038.wav|wˌaɪl ɪn wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɛvənti æt pˈæɹɪs juːdˈælɹɪk dʒˈɜːɹɪŋ ænd hɪz ɐsˈoʊsɪˌeɪts tˈɜːnd ˈaʊt ðə fˈɜːst bˈʊks pɹˈɪntᵻd ɪn fɹˈæns, ˈɔːlsoʊ ɪn ɹˈoʊmən kˈæɹɪktɚ.wˌaɪl ɪn fˈoːɹtiːn sˈɛvənti æt pˈæɹɪs juːdˈælɹɪk dʒˈɜːɹɪŋ ænd hɪz ɐsˈoʊsɪˌeɪts tˈɜːnd ˈaʊt ðə fˈɜːst bˈʊks pɹˈɪntᵻd ɪn fɹˈæns, ˈɔːlsoʊ ɪn ɹˈoʊmən kˈæɹɪktɚ. |0\n",
            "LJ001-0039.wav|ðə ɹˈoʊmən tˈaɪp ʌv ˈɔːl ðiːz pɹˈɪntɚz ɪz sˈɪmɪlɚɹ ɪn kˈæɹɪktɚ,ðə ɹˈoʊmən tˈaɪp ʌv ˈɔːl ðiːz pɹˈɪntɚz ɪz sˈɪmɪlɚɹ ɪn kˈæɹɪktɚ, |0\n",
            "LJ001-0040.wav|ænd ɪz vˈɛɹi sˈɪmpəl ænd lˈɛdʒəbəl, ænd ʌnɐfˈɛktᵻdli dɪzˈaɪnd fɔːɹ jˈuːs; bˌʌt ɪɾ ɪz baɪ nˈoʊ mˈiːnz wɪðˌaʊt bjˈuːɾi.ænd ɪz vˈɛɹi sˈɪmpəl ænd lˈɛdʒəbəl, ænd ʌnɐfˈɛktᵻdli dɪzˈaɪnd fɔːɹ jˈuːs; bˌʌt ɪɾ ɪz baɪ nˈoʊ mˈiːnz wɪðˌaʊt bjˈuːɾi. |0\n",
            "LJ001-0041.wav|ɪt mˈʌst biː sˈɛd ðˌɐɾɪt ɪz ɪn nˈoʊ wˈeɪ lˈaɪk ðə tɹænsˈɪʃən tˈaɪp ʌv sˌʌbɪˈɑːkoʊ,ɪt mˈʌst biː sˈɛd ðˌɐɾɪt ɪz ɪn nˈoʊ wˈeɪ lˈaɪk ðə tɹænsˈɪʃən tˈaɪp ʌv sˌʌbɪˈɑːkoʊ, |0\n",
            "LJ001-0042.wav|ænd ðˌoʊ mˈoːɹ ɹˈoʊmən ðɐn ðˈæt, jˈɛt skˈɛɹsli mˈoːɹ lˈaɪk ðə kəmplˈiːt ɹˈoʊmən tˈaɪp ʌvðɪ ˈɜːlɪɪst pɹˈɪntɚz ʌv ɹˈoʊm.ænd ðˌoʊ mˈoːɹ ɹˈoʊmən ðɐn ðˈæt, jˈɛt skˈɛɹsli mˈoːɹ lˈaɪk ðə kəmplˈiːt ɹˈoʊmən tˈaɪp ʌvðɪ ˈɜːlɪɪst pɹˈɪntɚz ʌv ɹˈoʊm. |0\n",
            "LJ001-0043.wav|ɐ fˈɜːðɚ dɪvˈɛləpmənt ʌvðə ɹˈoʊmən lˈɛɾɚ tˈʊk plˈeɪs æt vˈɛnɪs.ɐ fˈɜːðɚ dɪvˈɛləpmənt ʌvðə ɹˈoʊmən lˈɛɾɚ tˈʊk plˈeɪs æt vˈɛnɪs. |0\n",
            "LJ001-0044.wav|dʒˈɑːn ʌv spˈaɪɚz ænd hɪz bɹˈʌðɚ vˈɪndɪlˌɪn, fˈɑːloʊd baɪ nˈɪkələs dʒˈɛnsən, bɪɡˈæn tə pɹˈɪnt ɪn ðæt sˈɪɾi,dʒˈɑːn ʌv spˈaɪɚz ænd hɪz bɹˈʌðɚ vˈɪndɪlˌɪn, fˈɑːloʊd baɪ nˈɪkələs dʒˈɛnsən, bɪɡˈæn tə pɹˈɪnt ɪn ðæt sˈɪɾi, |0\n",
            "LJ001-0045.wav|wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti nˈaɪn, wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɛvənti;fˈoːɹtiːn sˈɪkstinˈaɪn, fˈoːɹtiːn sˈɛvənti; |0\n",
            "LJ001-0046.wav|ðɛɹ tˈaɪp ɪz ɔnðə lˈaɪnz ʌvðə dʒˈɜːmən ænd fɹˈɛntʃ ɹˈæðɚ ðɐn ʌvðə ɹˈoʊmən pɹˈɪntɚz.ðɛɹ tˈaɪp ɪz ɔnðə lˈaɪnz ʌvðə dʒˈɜːmən ænd fɹˈɛntʃ ɹˈæðɚ ðɐn ʌvðə ɹˈoʊmən pɹˈɪntɚz. |0\n",
            "LJ001-0047.wav|ʌv dʒˈɛnsən ɪt mˈʌst biː sˈɛd ðæt hiː kˈæɹid ðə dɪvˈɛləpmənt ʌv ɹˈoʊmən tˈaɪp æz fˈɑːɹ æz ɪt kæn ɡˈoʊ:ʌv dʒˈɛnsən ɪt mˈʌst biː sˈɛd ðæt hiː kˈæɹid ðə dɪvˈɛləpmənt ʌv ɹˈoʊmən tˈaɪp æz fˈɑːɹ æz ɪt kæn ɡˈoʊ: |0\n",
            "LJ001-0048.wav|hɪz lˈɛɾɚɹ ɪz ˈædmᵻɹəbli klˈɪɹ ænd ɹˈɛɡjʊlɚ, bˌʌt æt lˈiːst æz bjˈuːɾifəl æz ˌɛni ˈʌðɚ ɹˈoʊmən tˈaɪp.hɪz lˈɛɾɚɹ ɪz ˈædmᵻɹəbli klˈɪɹ ænd ɹˈɛɡjʊlɚ, bˌʌt æt lˈiːst æz bjˈuːɾifəl æz ˌɛni ˈʌðɚ ɹˈoʊmən tˈaɪp. |0\n",
            "LJ001-0049.wav|ˈæftɚ hɪz dˈɛθ ɪnðə \"fˈoːɹtiːn ˈeɪɾiz,\" ɔːɹ æt lˈiːst baɪ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd nˈaɪnti, pɹˈɪntɪŋ ɪn vˈɛnɪs hæd dᵻklˈaɪnd vˈɛɹi mˈʌtʃ;ˈæftɚ hɪz dˈɛθ ɪnðə \"fˈoːɹtiːn ˈeɪɾiz,\" ɔːɹ æt lˈiːst baɪ fˈoːɹtiːn nˈaɪnti, pɹˈɪntɪŋ ɪn vˈɛnɪs hæd dᵻklˈaɪnd vˈɛɹi mˈʌtʃ; |0\n",
            "LJ001-0050.wav|ænd ðˌoʊ ðə fˈeɪməs fˈæmɪli ʌv ˈɔːldəs ɹᵻstˈoːɹd ɪts tˈɛknɪkəl ˈɛksələns, ɹᵻdʒˈɛktɪŋ bˈæɾɚd lˈɛɾɚz,ænd ðˌoʊ ðə fˈeɪməs fˈæmɪli ʌv ˈɔːldəs ɹᵻstˈoːɹd ɪts tˈɛknɪkəl ˈɛksələns, ɹᵻdʒˈɛktɪŋ bˈæɾɚd lˈɛɾɚz, |0\n",
            "LJ001-0051.wav|ænd pˈeɪɪŋ ɡɹˈeɪt ɐtˈɛnʃən tə ðə \"pɹˈɛs wˈɜːk\" ɔːɹ ˈæktʃuːəl pɹˈɑːsɛs ʌv pɹˈɪntɪŋ,ænd pˈeɪɪŋ ɡɹˈeɪt ɐtˈɛnʃən tə ðə \"pɹˈɛs wˈɜːk\" ɔːɹ ˈæktʃuːəl pɹˈɑːsɛs ʌv pɹˈɪntɪŋ, |0\n",
            "LJ001-0052.wav|jˈɛt ðɛɹ tˈaɪp ɪz ɑːɹtˈɪstɪkli ˌɔn ɐ mˈʌtʃ lˈoʊɚ lˈɛvəl ðɐn dʒˈɛnsənz, ænd ɪn fˈækt jˈɛt ðɛɹ tˈaɪp ɪz ɑːɹtˈɪstɪkli ˌɔn ɐ mˈʌtʃ lˈoʊɚ lˈɛvəl ðɐn dʒˈɛnsənz, ænd ɪn fˈækt |0\n",
            "LJ001-0053.wav|ðeɪ mˈʌst biː kənsˈɪdɚd tə hæv ˈɛndᵻd ðɪ ˈeɪdʒ ʌv fˈaɪn pɹˈɪntɪŋ ɪn ˈɪɾəli.ðeɪ mˈʌst biː kənsˈɪdɚd tə hæv ˈɛndᵻd ðɪ ˈeɪdʒ ʌv fˈaɪn pɹˈɪntɪŋ ɪn ˈɪɾəli. |0\n",
            "LJ001-0054.wav|dʒˈɛnsən, haʊˈɛvɚ, hæd mˈɛni kəntˈɛmpɚɹˌɛɹiz hˌuː jˈuːzd bjˈuːɾifəl tˈaɪp,dʒˈɛnsən, haʊˈɛvɚ, hæd mˈɛni kəntˈɛmpɚɹˌɛɹiz hˌuː jˈuːzd bjˈuːɾifəl tˈaɪp, |0\n",
            "LJ001-0055.wav|sˌʌm ʌvwˈɪtʃ æz, ˈiː.dʒˈiː., ðæt ʌv dʒˈækɑːbəs ɹˈuːbɪəs ɔːɹ ʒˈæk lə ɹˈuːʒ ɪz skˈɛɹsli dɪstˈɪŋɡwɪʃəbəl fɹʌm hˈɪz.sˌʌm ʌvwˈɪtʃ æz, ˈiː.dʒˈiː., ðæt ʌv dʒˈækɑːbəs ɹˈuːbɪəs ɔːɹ ʒˈæk lə ɹˈuːʒ ɪz skˈɛɹsli dɪstˈɪŋɡwɪʃəbəl fɹʌm hˈɪz. |0\n",
            "LJ001-0056.wav|ɪt wʌz ðiːz ɡɹˈeɪt vᵻnˈiːʃən pɹˈɪntɚz, təɡˌɛðɚ wɪð ðɛɹ bɹˈɛðɹən ʌv ɹˈoʊm, mɪlˈæn,ɪt wʌz ðiːz ɡɹˈeɪt vᵻnˈiːʃən pɹˈɪntɚz, təɡˌɛðɚ wɪð ðɛɹ bɹˈɛðɹən ʌv ɹˈoʊm, mɪlˈæn, |0\n",
            "LJ001-0057.wav|pˈɑːɹmə, ænd wˈʌn ɔːɹ tˈuː ˈʌðɚ sˈɪɾiz, hˌuː pɹədˈuːst ðə splˈɛndɪd ᵻdˈɪʃənz ʌvðə klˈæsɪks, wˌɪtʃ ɑːɹ wˈʌn ʌvðə ɡɹˈeɪt ɡlˈoːɹiz ʌvðə pɹˈɪntɚz ˈɑːɹt,pˈɑːɹmə, ænd wˈʌn ɔːɹ tˈuː ˈʌðɚ sˈɪɾiz, hˌuː pɹədˈuːst ðə splˈɛndɪd ᵻdˈɪʃənz ʌvðə klˈæsɪks, wˌɪtʃ ɑːɹ wˈʌn ʌvðə ɡɹˈeɪt ɡlˈoːɹiz ʌvðə pɹˈɪntɚz ˈɑːɹt, |0\n",
            "LJ001-0058.wav|ænd ɑːɹ wˈɜːði ɹˌɛpɹᵻzˈɛntətˌɪvz ʌvðɪ ˈiːɡɚɹ ɛnθˈuːziˌæzəm fɚðə ɹᵻvˈaɪvd lˈɜːnɪŋ ʌv ðæt ˈɛpɑːk. baɪ fˈɑːɹ,ænd ɑːɹ wˈɜːði ɹˌɛpɹᵻzˈɛntətˌɪvz ʌvðɪ ˈiːɡɚɹ ɛnθˈuːziˌæzəm fɚðə ɹᵻvˈaɪvd lˈɜːnɪŋ ʌv ðæt ˈɛpɑːk. baɪ fˈɑːɹ, |0\n",
            "LJ001-0059.wav|ðə ɡɹˈeɪɾɚ pˈɑːɹt ʌv ðiːz ɪtˈæliən pɹˈɪntɚz, ɪt ʃˌʊd biː mˈɛnʃənd, wɜː dʒˈɜːmənz ɔːɹ fɹˈɛntʃmɛn, wˈɜːkɪŋ ˌʌndɚ ðɪ ˈɪnfluːəns ʌv ɪtˈæliən əpˈɪniən ænd ˈeɪmz.ðə ɡɹˈeɪɾɚ pˈɑːɹt ʌv ðiːz ɪtˈæliən pɹˈɪntɚz, ɪt ʃˌʊd biː mˈɛnʃənd, wɜː dʒˈɜːmənz ɔːɹ fɹˈɛntʃmɛn, wˈɜːkɪŋ ˌʌndɚ ðɪ ˈɪnfluːəns ʌv ɪtˈæliən əpˈɪniən ænd ˈeɪmz. |0\n",
            "LJ001-0060.wav|ɪt mˈʌst biː ˌʌndɚstˈʊd ðæt θɹuː ðə hˈoʊl ʌvðə fˈɪftiːnθ ænd ðə fˈɜːst kwˈɔːɹɾɚɹ ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹiz ɪt mˈʌst biː ˌʌndɚstˈʊd ðæt θɹuː ðə hˈoʊl ʌvðə fˈɪftiːnθ ænd ðə fˈɜːst kwˈɔːɹɾɚɹ ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹiz |0\n",
            "LJ001-0061.wav|ðə ɹˈoʊmən lˈɛɾɚ wʌz jˈuːzd sˈaɪd baɪ sˈaɪd wɪððə ɡˈɑːθɪk.ðə ɹˈoʊmən lˈɛɾɚ wʌz jˈuːzd sˈaɪd baɪ sˈaɪd wɪððə ɡˈɑːθɪk. |0\n",
            "LJ001-0062.wav|ˈiːvən ɪn ˈɪɾəli mˈoʊst əv ðə θiːəlˈɑːdʒɪkəl ænd lˈɔː bˈʊks wɜː pɹˈɪntᵻd ɪn ɡˈɑːθɪk lˈɛɾɚ,ˈiːvən ɪn ˈɪɾəli mˈoʊst əv ðə θiːəlˈɑːdʒɪkəl ænd lˈɔː bˈʊks wɜː pɹˈɪntᵻd ɪn ɡˈɑːθɪk lˈɛɾɚ, |0\n",
            "LJ001-0063.wav|wˌɪtʃ wʌz dʒˈɛnɚɹəli mˈoːɹ fˈɔːɹməli ɡˈɑːθɪk ðɐn ðə pɹˈɪntɪŋ ʌvðə dʒˈɜːmən wˈɜːkmɛn,wˌɪtʃ wʌz dʒˈɛnɚɹəli mˈoːɹ fˈɔːɹməli ɡˈɑːθɪk ðɐn ðə pɹˈɪntɪŋ ʌvðə dʒˈɜːmən wˈɜːkmɛn, |0\n",
            "LJ001-0064.wav|mˈɛnɪəv hˌuːz tˈaɪps, ˌɪndˈiːd, lˈaɪk ðæt ʌvðə sˌʌbɪˈɑːkoʊ wˈɜːks, ɑːɹ əvə tɹænsˈɪʃənəl kˈæɹɪktɚ.mˈɛnɪəv hˌuːz tˈaɪps, ˌɪndˈiːd, lˈaɪk ðæt ʌvðə sˌʌbɪˈɑːkoʊ wˈɜːks, ɑːɹ əvə tɹænsˈɪʃənəl kˈæɹɪktɚ. |0\n",
            "LJ001-0065.wav|ðɪs wʌz nˈoʊɾəbli ðə kˈeɪs wɪððɪ ˈɜːli wˈɜːks pɹˈɪntᵻd æɾ ˈʌlm, ænd ɪn ɐ sˈʌmwʌt lˈɛsɚ dᵻɡɹˈiː æɾ ˈɔːɡsbɜːɡ.ðɪs wʌz nˈoʊɾəbli ðə kˈeɪs wɪððɪ ˈɜːli wˈɜːks pɹˈɪntᵻd æɾ ˈʌlm, ænd ɪn ɐ sˈʌmwʌt lˈɛsɚ dᵻɡɹˈiː æɾ ˈɔːɡsbɜːɡ. |0\n",
            "LJ001-0066.wav|ɪn fˈækt ɡˈʌnθɚ zˈaɪnɚz fˈɜːst tˈaɪp (ˈæftɚwɚdz jˈuːzd baɪ ʃˈʌslɚ) ɪz ɹᵻmˈɑːɹkəbli lˈaɪk ðə tˈaɪp ʌvðə bᵻfˌoːɹmˈɛnʃənd sˌʌbɪˈɑːkoʊ bˈʊks.ɪn fˈækt ɡˈʌnθɚ zˈaɪnɚz fˈɜːst tˈaɪp (ˈæftɚwɚdz jˈuːzd baɪ ʃˈʌslɚ) ɪz ɹᵻmˈɑːɹkəbli lˈaɪk ðə tˈaɪp ʌvðə bᵻfˌoːɹmˈɛnʃənd sˌʌbɪˈɑːkoʊ bˈʊks. |0\n",
            "LJ001-0067.wav|ɪnðə lˈoʊ kˈʌntɹiz ænd kəlˈoʊn, wˌɪtʃ wɜː vˈɛɹi fˈɜːɾəl ʌv pɹˈɪntᵻd bˈʊks, ɡˈɑːθɪk wʌzðə fˈeɪvɚɹᵻt.ɪnðə lˈoʊ kˈʌntɹiz ænd kəlˈoʊn, wˌɪtʃ wɜː vˈɛɹi fˈɜːɾəl ʌv pɹˈɪntᵻd bˈʊks, ɡˈɑːθɪk wʌzðə fˈeɪvɚɹᵻt. |0\n",
            "LJ001-0068.wav|ðə kˌæɹɪktɚɹˈɪstɪk dˈʌtʃ tˈaɪp, æz ɹˌɛpɹᵻzˈɛntᵻd baɪ ðɪ ˈɛksələnt pɹˈɪntɚ dʒɚɹˈɑːɹd lˈiːw, ɪz vˈɛɹi pɹənˈaʊnst ænd ʌŋkˈɑːmpɹəmˌaɪzɪŋ ɡˈɑːθɪk.ðə kˌæɹɪktɚɹˈɪstɪk dˈʌtʃ tˈaɪp, æz ɹˌɛpɹᵻzˈɛntᵻd baɪ ðɪ ˈɛksələnt pɹˈɪntɚ dʒɚɹˈɑːɹd lˈiːw, ɪz vˈɛɹi pɹənˈaʊnst ænd ʌŋkˈɑːmpɹəmˌaɪzɪŋ ɡˈɑːθɪk. |0\n",
            "LJ001-0069.wav|ðɪs tˈaɪp wʌz ˌɪntɹədˈuːst ˌɪntʊ ˈɪŋɡlənd baɪ wˈɪŋkɪn də wˈɜːd, kˈækstənz səksˈɛsɚ,ðɪs tˈaɪp wʌz ˌɪntɹədˈuːst ˌɪntʊ ˈɪŋɡlənd baɪ wˈɪŋkɪn də wˈɜːd, kˈækstənz səksˈɛsɚ, |0\n",
            "LJ001-0070.wav|ænd wʌz jˈuːzd ðɛɹ wɪð vˈɛɹi lˈɪɾəl vˌɛɹɪˈeɪʃən ˈɔːl θɹuː ðə sˈɪkstiːnθ ænd sˈɛvəntˌiːnθ sˈɛntʃɚɹiz, ænd ˌɪndˈiːd ˌɪntʊ ðɪ ˈeɪtiːnθ.ænd wʌz jˈuːzd ðɛɹ wɪð vˈɛɹi lˈɪɾəl vˌɛɹɪˈeɪʃən ˈɔːl θɹuː ðə sˈɪkstiːnθ ænd sˈɛvəntˌiːnθ sˈɛntʃɚɹiz, ænd ˌɪndˈiːd ˌɪntʊ ðɪ ˈeɪtiːnθ. |0\n",
            "LJ001-0071.wav|mˈoʊst əv kˈækstənz ˈoʊn tˈaɪps ɑːɹ əvən ˈɜːlɪɚ kˈæɹɪktɚ, ðˌoʊ ðeɪ ˈɔːlsoʊ mˈʌtʃ ɹᵻzˈɛmbəl flˈɛmɪʃ ɔːɹ kəlˈoʊn lˈɛɾɚ.mˈoʊst əv kˈækstənz ˈoʊn tˈaɪps ɑːɹ əvən ˈɜːlɪɚ kˈæɹɪktɚ, ðˌoʊ ðeɪ ˈɔːlsoʊ mˈʌtʃ ɹᵻzˈɛmbəl flˈɛmɪʃ ɔːɹ kəlˈoʊn lˈɛɾɚ. |0\n",
            "LJ001-0072.wav|ˈæftɚ ðɪ ˈɛnd ʌvðə fˈɪftiːnθ sˈɛntʃɚɹi ðə dᵻɡɹɐdˈeɪʃən ʌv pɹˈɪntɪŋ, ɪspˈɛʃəli ɪn dʒˈɜːməni ænd ˈɪɾəli,ˈæftɚ ðɪ ˈɛnd ʌvðə fˈɪftiːnθ sˈɛntʃɚɹi ðə dᵻɡɹɐdˈeɪʃən ʌv pɹˈɪntɪŋ, ɪspˈɛʃəli ɪn dʒˈɜːməni ænd ˈɪɾəli, |0\n",
            "LJ001-0073.wav|wɛnt ˌɔn ɐpˈeɪs; ænd baɪ ðɪ ˈɛnd ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹi ðɛɹwˌʌz nˈoʊ ɹˈiəli bjˈuːɾifəl pɹˈɪntɪŋ dˈʌn:wɛnt ˌɔn ɐpˈeɪs; ænd baɪ ðɪ ˈɛnd ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹi ðɛɹwˌʌz nˈoʊ ɹˈiəli bjˈuːɾifəl pɹˈɪntɪŋ dˈʌn: |0\n",
            "LJ001-0074.wav|ðə bˈɛst, mˈoʊstli fɹˈɛntʃ ɔːɹ lˈoʊkˈʌntɹi, wʌz nˈiːt ænd klˈɪɹ, bˌʌt wɪðˌaʊt ˌɛni dɪstˈɪŋkʃən;ðə bˈɛst, mˈoʊstli fɹˈɛntʃ ɔːɹ lˈoʊkˈʌntɹi, wʌz nˈiːt ænd klˈɪɹ, bˌʌt wɪðˌaʊt ˌɛni dɪstˈɪŋkʃən; |0\n",
            "LJ001-0075.wav|ðə wˈɜːst, wˌɪtʃ pɚhˈæps wʌzðɪ ˈɪŋɡlɪʃ, wʌzɐ tˈɛɹᵻbəl fˈɔːlɪŋˈɔf fɹʌmðə wˈɜːk ʌvðɪ ˈɜːlɪɚ pɹˈɛsᵻz;ðə wˈɜːst, wˌɪtʃ pɚhˈæps wʌzðɪ ˈɪŋɡlɪʃ, wʌzɐ tˈɛɹᵻbəl fˈɔːlɪŋˈɔf fɹʌmðə wˈɜːk ʌvðɪ ˈɜːlɪɚ pɹˈɛsᵻz; |0\n",
            "LJ001-0076.wav|ænd θˈɪŋz ɡɑːt wˈɜːs ænd wˈɜːs θɹuː ðə hˈoʊl ʌvðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi, sˌoʊ ðæt ɪnðɪ ˈeɪtiːnθ pɹˈɪntɪŋ wʌz vˈɛɹi mˈɪzɹəbli pɚfˈɔːɹmd.ænd θˈɪŋz ɡɑːt wˈɜːs ænd wˈɜːs θɹuː ðə hˈoʊl ʌvðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi, sˌoʊ ðæt ɪnðɪ ˈeɪtiːnθ pɹˈɪntɪŋ wʌz vˈɛɹi mˈɪzɹəbli pɚfˈɔːɹmd. |0\n",
            "LJ001-0077.wav|ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn wˈʌn θˈaʊzənd sˈɛvənhˈʌndɹɪd twˈɛnti)ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn sˈɛvəntˌiːn twˈɛnti) |0\n",
            "LJ001-0078.wav|tʊ ɪmpɹˈuːv ðə lˈɛɾɚɹ ɪn fˈɔːɹm.tʊ ɪmpɹˈuːv ðə lˈɛɾɚɹ ɪn fˈɔːɹm. |0\n",
            "LJ001-0079.wav|kˈæslɑːnz tˈaɪp ɪz klˈɪɹ ænd nˈiːt, ænd fˈɛɹli wˈɛl dɪzˈaɪnd;kˈæslɑːnz tˈaɪp ɪz klˈɪɹ ænd nˈiːt, ænd fˈɛɹli wˈɛl dɪzˈaɪnd; |0\n",
            "LJ001-0080.wav|hiː sˈiːmz tə hæv tˈeɪkən ðə lˈɛɾɚɹ ʌvðɪ ˈɛlzɪvˌɪɹz ʌvðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi fɔːɹ hɪz mˈɑːdəl:hiː sˈiːmz tə hæv tˈeɪkən ðə lˈɛɾɚɹ ʌvðɪ ˈɛlzɪvˌɪɹz ʌvðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi fɔːɹ hɪz mˈɑːdəl: |0\n",
            "LJ001-0081.wav|tˈaɪp kˈæst fɹʌm hɪz mˈeɪtɹɪsˌiːz ɪz stˈɪl ɪn ˈɛvɹɪdˌeɪ jˈuːs.tˈaɪp kˈæst fɹʌm hɪz mˈeɪtɹɪsˌiːz ɪz stˈɪl ɪn ˈɛvɹɪdˌeɪ jˈuːs. |0\n",
            "LJ001-0082.wav|ɪn spˈaɪt, haʊˈɛvɚ, ʌv hɪz pɹˈeɪzwɜːði ˈɛfɚts, pɹˈɪntɪŋ hæd stˈɪl wˈʌn lˈæst dᵻɡɹɐdˈeɪʃən tʊ ˌʌndɚɡˈoʊ.ɪn spˈaɪt, haʊˈɛvɚ, ʌv hɪz pɹˈeɪzwɜːði ˈɛfɚts, pɹˈɪntɪŋ hæd stˈɪl wˈʌn lˈæst dᵻɡɹɐdˈeɪʃən tʊ ˌʌndɚɡˈoʊ. |0\n",
            "LJ001-0083.wav|ðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi fˈaʊnts wɜː bˈæd ɹˈæðɚ nˈɛɡətˌɪvli ðɐn pˈɑːzᵻtˌɪvli.ðə sˈɛvəntˌiːnθ sˈɛntʃɚɹi fˈaʊnts wɜː bˈæd ɹˈæðɚ nˈɛɡətˌɪvli ðɐn pˈɑːzᵻtˌɪvli. |0\n",
            "LJ001-0084.wav|bˌʌt fɚðə bjˈuːɾi ʌvðɪ ˈɜːlɪɚ wˈɜːk ðeɪ mˌaɪthɐv sˈiːmd tˈɑːlɚɹəbəl.bˌʌt fɚðə bjˈuːɾi ʌvðɪ ˈɜːlɪɚ wˈɜːk ðeɪ mˌaɪthɐv sˈiːmd tˈɑːlɚɹəbəl. |0\n",
            "LJ001-0085.wav|ɪt wʌz ɹɪzˈɜːvd fɚðə fˈaʊndɚz ʌvðə lˈeɪɾɚɹ ˈeɪtiːnθ sˈɛntʃɚɹi tə pɹədˈuːs lˈɛɾɚz wˌɪtʃ ɑːɹ pˈɑːzᵻtˌɪvli ˈʌɡli, ænd wˈɪtʃ, ɪt mˈeɪ biː ˈædᵻd,ɪt wʌz ɹɪzˈɜːvd fɚðə fˈaʊndɚz ʌvðə lˈeɪɾɚɹ ˈeɪtiːnθ sˈɛntʃɚɹi tə pɹədˈuːs lˈɛɾɚz wˌɪtʃ ɑːɹ pˈɑːzᵻtˌɪvli ˈʌɡli, ænd wˈɪtʃ, ɪt mˈeɪ biː ˈædᵻd, |0\n",
            "LJ001-0086.wav|ɑːɹ dˈæzlɪŋ ænd ʌnplˈɛzənt tə ðɪ ˈaɪ ˈoʊɪŋ tə ðə klˈʌmzi θˈɪkənɪŋ ænd vˈʌlɡɚ θˈɪnɪŋ ʌvðə lˈaɪnz:ɑːɹ dˈæzlɪŋ ænd ʌnplˈɛzənt tə ðɪ ˈaɪ ˈoʊɪŋ tə ðə klˈʌmzi θˈɪkənɪŋ ænd vˈʌlɡɚ θˈɪnɪŋ ʌvðə lˈaɪnz: |0\n",
            "LJ001-0087.wav|fɚðə sˈɛvəntˌiːnθsˈɛntʃɚɹi lˈɛɾɚz ɑːɹ æt lˈiːst pjˈʊɹ ænd sˈɪmpəl ɪn lˈaɪn. ðɪ ɪtˈæliən, bədˈoʊni, ænd ðə fɹˈɛntʃmən, dˈɪdɑːt,fɚðə sˈɛvəntˌiːnθsˈɛntʃɚɹi lˈɛɾɚz ɑːɹ æt lˈiːst pjˈʊɹ ænd sˈɪmpəl ɪn lˈaɪn. ðɪ ɪtˈæliən, bədˈoʊni, ænd ðə fɹˈɛntʃmən, dˈɪdɑːt, |0\n",
            "LJ001-0088.wav|wɜː ðə lˈiːdɚz ɪn ðɪs lˈʌkləs tʃˈeɪndʒ, ðˌoʊ ˌaʊɚɹ ˈoʊn bˈæskɚvˌɪl, hˌuː wʌz æt wˈɜːk sˌʌm jˈɪɹz bᵻfˈoːɹ ðˌɛm, wɛnt mˈʌtʃ ɔnðə sˈeɪm lˈaɪnz;wɜː ðə lˈiːdɚz ɪn ðɪs lˈʌkləs tʃˈeɪndʒ, ðˌoʊ ˌaʊɚɹ ˈoʊn bˈæskɚvˌɪl, hˌuː wʌz æt wˈɜːk sˌʌm jˈɪɹz bᵻfˈoːɹ ðˌɛm, wɛnt mˈʌtʃ ɔnðə sˈeɪm lˈaɪnz; |0\n",
            "LJ001-0089.wav|bˌʌt hɪz lˈɛɾɚz, ðˌoʊ ʌnˈɪntɚɹəstɪŋ ænd pˈʊɹ, ɑːɹ nˌɑːt nˌɪɹli sˌoʊ ɡɹˈoʊs ænd vˈʌlɡɚɹ æz ðoʊz ʌv ˈiːðɚ ðɪ ɪtˈæliən ɔːɹ ðə fɹˈɛntʃmən.bˌʌt hɪz lˈɛɾɚz, ðˌoʊ ʌnˈɪntɚɹəstɪŋ ænd pˈʊɹ, ɑːɹ nˌɑːt nˌɪɹli sˌoʊ ɡɹˈoʊs ænd vˈʌlɡɚɹ æz ðoʊz ʌv ˈiːðɚ ðɪ ɪtˈæliən ɔːɹ ðə fɹˈɛntʃmən. |0\n",
            "LJ001-0090.wav|wɪð ðɪs tʃˈeɪndʒ ðɪ ˈɑːɹt ʌv pɹˈɪntɪŋ tˈʌtʃt bˈɑːɾəm,wɪð ðɪs tʃˈeɪndʒ ðɪ ˈɑːɹt ʌv pɹˈɪntɪŋ tˈʌtʃt bˈɑːɾəm, |0\n",
            "LJ001-0091.wav|sˈoʊ fˌɑːɹ æz fˈaɪn pɹˈɪntɪŋ ɪz kənsˈɜːnd, ðˌoʊ pˈeɪpɚ dɪdnˌɑːt ɡɛt tʊ ɪts wˈɜːst tˈɪl ɐbˌaʊt wˈʌn θˈaʊzənd ˈeɪthˈʌndɹɪd fˈoːɹɾi.sˈoʊ fˌɑːɹ æz fˈaɪn pɹˈɪntɪŋ ɪz kənsˈɜːnd, ðˌoʊ pˈeɪpɚ dɪdnˌɑːt ɡɛt tʊ ɪts wˈɜːst tˈɪl ɐbˌaʊt ˈeɪtiːn fˈɔːɹɾi. |0\n",
            "LJ001-0092.wav|ðə tʃˈɪzɪk pɹˈɛs ɪn wˈʌn θˈaʊzənd ˈeɪthˈʌndɹɪd fˈoːɹɾi fˈoːɹ ɹᵻvˈaɪvd kˈæslɑːnz fˈaʊnts, pɹˈɪntɪŋ fɔːɹ mˈɛsrz. lˈɔŋmən ðə dˈaɪɚɹi ʌv lˈeɪdi wˈɪlʌfbi.ðə tʃˈɪzɪk pɹˈɛs ɪn ˈeɪtiːn fˈɔːɹɾifˈoːɹ ɹᵻvˈaɪvd kˈæslɑːnz fˈaʊnts, pɹˈɪntɪŋ fɔːɹ mˈɛsrz. lˈɔŋmən ðə dˈaɪɚɹi ʌv lˈeɪdi wˈɪlʌfbi. |0\n",
            "LJ001-0093.wav|ðɪs ɛkspˈɛɹɪmənt wʌz sˈoʊ fˌɑːɹ səksˈɛsfəl ðæt ɐbˌaʊt wˈʌn θˈaʊzənd ˈeɪthˈʌndɹɪd fˈɪfti mˈɛsrz. mˈɪlɚɹ ænd ɹˈɪtʃɚd ʌv ˈɛdɪnbʌɹə ðɪs ɛkspˈɛɹɪmənt wʌz sˈoʊ fˌɑːɹ səksˈɛsfəl ðæt ɐbˌaʊt ˈeɪtiːn fˈɪfti mˈɛsrz. mˈɪlɚɹ ænd ɹˈɪtʃɚd ʌv ˈɛdɪnbʌɹə |0\n",
            "LJ001-0094.wav|wɜːɹ ɪndˈuːst tə kˈʌt pˈʌntʃᵻz fɚɹə sˈɪɹiz ʌv \"ˈoʊld stˈaɪl\" lˈɛɾɚz.wɜːɹ ɪndˈuːst tə kˈʌt pˈʌntʃᵻz fɚɹə sˈɪɹiz ʌv \"ˈoʊld stˈaɪl\" lˈɛɾɚz. |0\n",
            "LJ001-0095.wav|ðiːz ænd sˈɪmɪlɚ fˈaʊnts, kˈæst baɪ ðɪ əbˌʌv fˈɜːm ænd ˈʌðɚz,ðiːz ænd sˈɪmɪlɚ fˈaʊnts, kˈæst baɪ ðɪ əbˌʌv fˈɜːm ænd ˈʌðɚz, |0\n",
            "LJ001-0096.wav|hæv nˈaʊ kˈʌm ˌɪntʊ dʒˈɛnɚɹəl jˈuːs ænd ɑːɹ ˈɑːbviəsli ɐ ɡɹˈeɪt ɪmpɹˈuːvmənt ɔnðɪ ˈɔːɹdɪnˌɛɹi \"mˈɑːdɚn stˈaɪl\" ɪn jˈuːs ɪn ˈɪŋɡlənd, wˌɪtʃ ɪz ɪn fˈækt ðə bədˈoʊni tˈaɪp hæv nˈaʊ kˈʌm ˌɪntʊ dʒˈɛnɚɹəl jˈuːs ænd ɑːɹ ˈɑːbviəsli ɐ ɡɹˈeɪt ɪmpɹˈuːvmənt ɔnðɪ ˈɔːɹdɪnˌɛɹi \"mˈɑːdɚn stˈaɪl\" ɪn jˈuːs ɪn ˈɪŋɡlənd, wˌɪtʃ ɪz ɪn fˈækt ðə bədˈoʊni tˈaɪp |0\n",
            "LJ001-0097.wav|ɐ lˈɪɾəl ɹᵻdˈuːst ɪn ˈʌɡlinəs. ðə dɪzˈaɪn ʌvðə lˈɛɾɚz ʌv ðɪs mˈɑːdɚn \"ˈoʊld stˈaɪl\" lˈiːvz ɐ ɡˈʊd dˈiːl təbi dɪzˈaɪɚd,ɐ lˈɪɾəl ɹᵻdˈuːst ɪn ˈʌɡlinəs. ðə dɪzˈaɪn ʌvðə lˈɛɾɚz ʌv ðɪs mˈɑːdɚn \"ˈoʊld stˈaɪl\" lˈiːvz ɐ ɡˈʊd dˈiːl təbi dɪzˈaɪɚd, |0\n",
            "LJ001-0098.wav|ænd ðə hˈoʊl ɪfˈɛkt ɪz ɐ lˈɪɾəl tˈuː ɡɹˈeɪ, ˈoʊɪŋ tə ðə θˈɪnnəs ʌvðə lˈɛɾɚz.ænd ðə hˈoʊl ɪfˈɛkt ɪz ɐ lˈɪɾəl tˈuː ɡɹˈeɪ, ˈoʊɪŋ tə ðə θˈɪnnəs ʌvðə lˈɛɾɚz. |0\n",
            "LJ001-0099.wav|ɪt mˈʌst biː ɹᵻmˈɛmbɚd, haʊˈɛvɚ, ðæt mˈoʊst mˈɑːdɚn pɹˈɪntɪŋ ɪz dˈʌn baɪ məʃˈiːnɚɹi ˌɔn sˈɔft pˈeɪpɚ, ænd nˌɑːt baɪ ðə hˈænd pɹˈɛs,ɪt mˈʌst biː ɹᵻmˈɛmbɚd, haʊˈɛvɚ, ðæt mˈoʊst mˈɑːdɚn pɹˈɪntɪŋ ɪz dˈʌn baɪ məʃˈiːnɚɹi ˌɔn sˈɔft pˈeɪpɚ, ænd nˌɑːt baɪ ðə hˈænd pɹˈɛs, |0\n",
            "LJ001-0100.wav|ænd ðiːz sˈʌmwʌt wˈaɪɚɹi lˈɛɾɚz ɑːɹ sˈuːɾəbəl fɚðə məʃˈiːn pɹˈɑːsɛs, wˌɪtʃ wʊd nˌɑːt dˈuː dʒˈʌstɪs tə lˈɛɾɚz ʌv mˈoːɹ dʒˈɛnɚɹəs dɪzˈaɪn.ænd ðiːz sˈʌmwʌt wˈaɪɚɹi lˈɛɾɚz ɑːɹ sˈuːɾəbəl fɚðə məʃˈiːn pɹˈɑːsɛs, wˌɪtʃ wʊd nˌɑːt dˈuː dʒˈʌstɪs tə lˈɛɾɚz ʌv mˈoːɹ dʒˈɛnɚɹəs dɪzˈaɪn. |0\n",
            "LJ001-0101.wav|ɪɾ ɪz dɪskˈɜːɹɪdʒɪŋ tə nˈoʊt ðætðɪ ɪmpɹˈuːvmənt ʌvðə lˈæst fˈɪfti jˈɪɹz ɪz ˈɔːlmoʊst hˈoʊli kənfˈaɪnd tə ɡɹˈeɪt bɹˈɪtən.ɪɾ ɪz dɪskˈɜːɹɪdʒɪŋ tə nˈoʊt ðætðɪ ɪmpɹˈuːvmənt ʌvðə lˈæst fˈɪfti jˈɪɹz ɪz ˈɔːlmoʊst hˈoʊli kənfˈaɪnd tə ɡɹˈeɪt bɹˈɪtən. |0\n",
            "LJ001-0102.wav|hˈɪɹ ɐnd ðˈɛɹ ɐ bˈʊk ɪz pɹˈɪntᵻd ɪn fɹˈæns ɔːɹ dʒˈɜːməni wɪð sˌʌm pɹɪtˈɛnʃən tə ɡˈʊd tˈeɪst,hˈɪɹ ɐnd ðˈɛɹ ɐ bˈʊk ɪz pɹˈɪntᵻd ɪn fɹˈæns ɔːɹ dʒˈɜːməni wɪð sˌʌm pɹɪtˈɛnʃən tə ɡˈʊd tˈeɪst, |0\n",
            "LJ001-0103.wav|bˌʌt ðə dʒˈɛnɚɹəl ɹᵻvˈaɪvəl ʌvðɪ ˈoʊld fˈɔːɹmz hɐz mˌeɪd nˈoʊ wˈeɪ ɪn ðoʊz kˈʌntɹiz.bˌʌt ðə dʒˈɛnɚɹəl ɹᵻvˈaɪvəl ʌvðɪ ˈoʊld fˈɔːɹmz hɐz mˌeɪd nˈoʊ wˈeɪ ɪn ðoʊz kˈʌntɹiz. |0\n",
            "LJ001-0104.wav|ˈɪɾəli ɪz kəntˈɛntᵻdli stˈæɡnənt.ˈɪɾəli ɪz kəntˈɛntᵻdli stˈæɡnənt. |0\n",
            "LJ001-0105.wav|ɐmˈɛɹɪkə hɐz pɹədˈuːst ɐ ɡˈʊd mˈɛni ʃˈoʊi bˈʊks, ðə taɪpˈɑːɡɹəfi, pˈeɪpɚ, ænd ˌɪləstɹˈeɪʃənz ʌvwˈɪtʃ ɑːɹ, haʊˈɛvɚ, ˈɔːl ɹˈɔŋ,ɐmˈɛɹɪkə hɐz pɹədˈuːst ɐ ɡˈʊd mˈɛni ʃˈoʊi bˈʊks, ðə taɪpˈɑːɡɹəfi, pˈeɪpɚ, ænd ˌɪləstɹˈeɪʃənz ʌvwˈɪtʃ ɑːɹ, haʊˈɛvɚ, ˈɔːl ɹˈɔŋ, |0\n",
            "LJ001-0106.wav|ˈɑːdᵻɾi ɹˈæðɚ ðɐn ɹˈæʃənəl bjˈuːɾi ænd mˈiːnɪŋ bˌiːɪŋ ɐpˈæɹəntli ðə θˈɪŋ sˈɔːt fɔːɹ bˈoʊθ ɪnðə lˈɛɾɚz ænd ðɪ ˌɪləstɹˈeɪʃənz.ˈɑːdᵻɾi ɹˈæðɚ ðɐn ɹˈæʃənəl bjˈuːɾi ænd mˈiːnɪŋ bˌiːɪŋ ɐpˈæɹəntli ðə θˈɪŋ sˈɔːt fɔːɹ bˈoʊθ ɪnðə lˈɛɾɚz ænd ðɪ ˌɪləstɹˈeɪʃənz. |0\n",
            "LJ001-0107.wav|tə sˈeɪ ɐ fjˈuː wˈɜːdz ɔnðə pɹˈɪnsɪpəlz ʌv dɪzˈaɪn ɪn taɪpˈɑːɡɹəfi:tə sˈeɪ ɐ fjˈuː wˈɜːdz ɔnðə pɹˈɪnsɪpəlz ʌv dɪzˈaɪn ɪn taɪpˈɑːɡɹəfi: |0\n",
            "LJ001-0108.wav|ɪɾ ɪz ˈɑːbviəs ðæt lˌɛdʒəbˈɪlᵻɾi ɪz ðə fˈɜːst θˈɪŋ təbi ˈeɪmd æɾ ɪnðə fˈɔːɹmz ʌvðə lˈɛɾɚz;ɪɾ ɪz ˈɑːbviəs ðæt lˌɛdʒəbˈɪlᵻɾi ɪz ðə fˈɜːst θˈɪŋ təbi ˈeɪmd æɾ ɪnðə fˈɔːɹmz ʌvðə lˈɛɾɚz; |0\n",
            "LJ001-0109.wav|ðɪs ɪz bˈɛst fˈɜːðɚd baɪ ðɪ ɐvˈɔɪdəns ʌv ɪɹˈæʃənəl swˈɛlɪŋz ænd spˈaɪki pɹədʒˈɛkʃənz, ænd baɪ ðə jˈuːzɪŋ ʌv kˈɛɹfəl pjˈʊɹɹᵻɾi ʌv lˈaɪn.ðɪs ɪz bˈɛst fˈɜːðɚd baɪ ðɪ ɐvˈɔɪdəns ʌv ɪɹˈæʃənəl swˈɛlɪŋz ænd spˈaɪki pɹədʒˈɛkʃənz, ænd baɪ ðə jˈuːzɪŋ ʌv kˈɛɹfəl pjˈʊɹɹᵻɾi ʌv lˈaɪn. |0\n",
            "LJ001-0110.wav|ˈiːvən ðə kˈæslɑːn tˈaɪp wɛn ɛnlˈɑːɹdʒd ʃˈoʊz ɡɹˈeɪt ʃˈɔːɹtkʌmɪŋz ɪn ðɪs ɹᵻspˈɛkt:ˈiːvən ðə kˈæslɑːn tˈaɪp wɛn ɛnlˈɑːɹdʒd ʃˈoʊz ɡɹˈeɪt ʃˈɔːɹtkʌmɪŋz ɪn ðɪs ɹᵻspˈɛkt: |0\n",
            "LJ001-0111.wav|ðɪ ˈɛndz ʌv mˈɛnɪəv ðə lˈɛɾɚz sˈʌtʃ ɐz ðə tˈiː ænd ˈiː ɑːɹ hˈʊkt ˌʌp ɪn ɐ vˈʌlɡɚ ænd mˈiːnɪŋləs wˈeɪ,ðɪ ˈɛndz ʌv mˈɛnɪəv ðə lˈɛɾɚz sˈʌtʃ ɐz ðə tˈiː ænd ˈiː ɑːɹ hˈʊkt ˌʌp ɪn ɐ vˈʌlɡɚ ænd mˈiːnɪŋləs wˈeɪ, |0\n",
            "LJ001-0112.wav|ɪnstˈɛd ʌv ˈɛndɪŋ ɪnðə ʃˈɑːɹp ænd klˈɪɹ stɹˈoʊk ʌv dʒˈɛnsənz lˈɛɾɚz;ɪnstˈɛd ʌv ˈɛndɪŋ ɪnðə ʃˈɑːɹp ænd klˈɪɹ stɹˈoʊk ʌv dʒˈɛnsənz lˈɛɾɚz; |0\n",
            "LJ001-0113.wav|ðɛɹ ɪz ɐ ɡɹˈoʊsnəs ɪnðɪ ˌʌpɚ fˈɪnɪʃɪŋz ʌv lˈɛɾɚz lˈaɪk ðə sˈiː, ðɪ ˈeɪ, ænd sˌoʊ ˈɔn,ðɛɹ ɪz ɐ ɡɹˈoʊsnəs ɪnðɪ ˌʌpɚ fˈɪnɪʃɪŋz ʌv lˈɛɾɚz lˈaɪk ðə sˈiː, ðɪ ˈeɪ, ænd sˌoʊ ˈɔn, |0\n",
            "LJ001-0114.wav|ɐn ˈʌɡli pˈɛɹʃˈeɪpt swˈɛlɪŋ dᵻfˈeɪsɪŋ ðə fˈɔːɹm ʌvðə lˈɛɾɚ:ɐn ˈʌɡli pˈɛɹʃˈeɪpt swˈɛlɪŋ dᵻfˈeɪsɪŋ ðə fˈɔːɹm ʌvðə lˈɛɾɚ: |0\n",
            "LJ001-0115.wav|ɪn ʃˈɔːɹt, ɪt hˈæpənz tə ðɪs kɹˈæft, æz tʊ ˈʌðɚz, ðætðə jˌuːɾɪlɪtˈɛɹiən pɹˈæktɪs, ðˌoʊ ɪt pɹəfˈɛsᵻz tʊ ɐvˈɔɪd ˈɔːɹnəmənt,ɪn ʃˈɔːɹt, ɪt hˈæpənz tə ðɪs kɹˈæft, æz tʊ ˈʌðɚz, ðætðə jˌuːɾɪlɪtˈɛɹiən pɹˈæktɪs, ðˌoʊ ɪt pɹəfˈɛsᵻz tʊ ɐvˈɔɪd ˈɔːɹnəmənt, |0\n",
            "LJ001-0116.wav|stˈɪl klˈɪŋz tʊ ɐ fˈuːlɪʃ, bɪkˈʌz mɪsˌʌndɚstˈʊd kənvˌɛnʃənˈælᵻɾi, dᵻdˈuːst fɹʌm wʌt wʌz wˈʌns ˈɔːɹnəmənt, ænd ɪz baɪ nˈoʊ mˈiːnz jˈuːsfəl;stˈɪl klˈɪŋz tʊ ɐ fˈuːlɪʃ, bɪkˈʌz mɪsˌʌndɚstˈʊd kənvˌɛnʃənˈælᵻɾi, dᵻdˈuːst fɹʌm wʌt wʌz wˈʌns ˈɔːɹnəmənt, ænd ɪz baɪ nˈoʊ mˈiːnz jˈuːsfəl; |0\n",
            "LJ001-0117.wav|wˌɪtʃ tˈaɪɾəl kæn ˈoʊnli biː klˈeɪmd baɪ ɑːɹtˈɪstɪk pɹˈæktɪs, wˈɛðɚ ðɪ ˈɑːɹt ɪn ɪt biː kˈɑːnʃəs ɔːɹ ʌŋkˈɑːnʃəs.wˌɪtʃ tˈaɪɾəl kæn ˈoʊnli biː klˈeɪmd baɪ ɑːɹtˈɪstɪk pɹˈæktɪs, wˈɛðɚ ðɪ ˈɑːɹt ɪn ɪt biː kˈɑːnʃəs ɔːɹ ʌŋkˈɑːnʃəs. |0\n",
            "LJ001-0118.wav|ɪn nˈoʊ kˈæɹɪktɚz ɪz ðə kˈɑːntɹæst bᵻtwˌiːn ðɪ ˈʌɡli ænd vˈʌlɡɚɹ ɪlˌɛdʒəbˈɪlᵻɾi ʌvðə mˈɑːdɚn tˈaɪp ɪn nˈoʊ kˈæɹɪktɚz ɪz ðə kˈɑːntɹæst bᵻtwˌiːn ðɪ ˈʌɡli ænd vˈʌlɡɚɹ ɪlˌɛdʒəbˈɪlᵻɾi ʌvðə mˈɑːdɚn tˈaɪp |0\n",
            "LJ001-0119.wav|ænd ðɪ ˈɛlɪɡəns ænd lˌɛdʒəbˈɪlᵻɾi ʌvðɪ ˈeɪntʃənt mˈoːɹ stɹˈaɪkɪŋ ðɐn ɪnðɪ ˈæɹɐbˌɪk nˈuːmɚɹəlz.ænd ðɪ ˈɛlɪɡəns ænd lˌɛdʒəbˈɪlᵻɾi ʌvðɪ ˈeɪntʃənt mˈoːɹ stɹˈaɪkɪŋ ðɐn ɪnðɪ ˈæɹɐbˌɪk nˈuːmɚɹəlz. |0\n",
            "LJ001-0120.wav|ɪnðɪ ˈoʊld pɹˈɪnt ˈiːtʃ fˈɪɡjɚ hɐz ɪts dˈɛfɪnət ˌɪndᵻvˌɪduːˈælᵻɾi, ænd wˈʌn kænˈɑːt biː mɪstˈeɪkən fɚðɪ ˈʌðɚ;ɪnðɪ ˈoʊld pɹˈɪnt ˈiːtʃ fˈɪɡjɚ hɐz ɪts dˈɛfɪnət ˌɪndᵻvˌɪduːˈælᵻɾi, ænd wˈʌn kænˈɑːt biː mɪstˈeɪkən fɚðɪ ˈʌðɚ; |0\n",
            "LJ001-0121.wav|ɪn ɹˈiːdɪŋ ðə mˈɑːdɚn fˈɪɡjɚz ðɪ ˈaɪz mˈʌst biː stɹˈeɪnd bᵻfˌoːɹ ðə ɹˈiːdɚ kæn hæv ˌɛni ɹˈiːzənəbəl əʃˈʊɹɹəns ɪn ɹˈiːdɪŋ ðə mˈɑːdɚn fˈɪɡjɚz ðɪ ˈaɪz mˈʌst biː stɹˈeɪnd bᵻfˌoːɹ ðə ɹˈiːdɚ kæn hæv ˌɛni ɹˈiːzənəbəl əʃˈʊɹɹəns |0\n",
            "LJ001-0122.wav|ðæt hiː hɐz ɐ fˈaɪv, ɐn ˈeɪt, ɔːɹ ɐ θɹˈiː bᵻfˈoːɹ hˌɪm, ʌnlˈɛs ðə pɹˈɛs wˈɜːk ɪz ʌvðə bˈɛst:ðæt hiː hɐz ɐ fˈaɪv, ɐn ˈeɪt, ɔːɹ ɐ θɹˈiː bᵻfˈoːɹ hˌɪm, ʌnlˈɛs ðə pɹˈɛs wˈɜːk ɪz ʌvðə bˈɛst: |0\n",
            "LJ001-0123.wav|ðɪs ɪz ˈɔːkwɚd ɪf juː hæv tə ɹˈɛd bɹˈædʃɔːz ɡˈaɪd ɪn ɐ hˈɜːɹi.ðɪs ɪz ˈɔːkwɚd ɪf juː hæv tə ɹˈɛd bɹˈædʃɔːz ɡˈaɪd ɪn ɐ hˈɜːɹi. |0\n",
            "LJ001-0124.wav|wˈʌn ʌvðə dˈɪfɹənsᵻz bᵻtwˌiːn ðə fˈaɪn tˈaɪp ænd ðə jˌuːɾɪlɪtˈɛɹiən mˈʌst pɹˈɑːbəbli biː pˌʊt dˌaʊn tʊ ɐ mɪsˌæpɹihˈɛnʃən əvə kəmˈɜːʃəl nəsˈɛsɪɾi:wˈʌn ʌvðə dˈɪfɹənsᵻz bᵻtwˌiːn ðə fˈaɪn tˈaɪp ænd ðə jˌuːɾɪlɪtˈɛɹiən mˈʌst pɹˈɑːbəbli biː pˌʊt dˌaʊn tʊ ɐ mɪsˌæpɹihˈɛnʃən əvə kəmˈɜːʃəl nəsˈɛsɪɾi: |0\n",
            "LJ001-0125.wav|ðɪs ɪz ðə nˈæɹoʊɪŋ ʌvðə mˈɑːdɚn lˈɛɾɚz.ðɪs ɪz ðə nˈæɹoʊɪŋ ʌvðə mˈɑːdɚn lˈɛɾɚz. |0\n",
            "LJ001-0126.wav|mˈoʊst əv dʒˈɛnsənz lˈɛɾɚz ɑːɹ dɪzˈaɪnd wɪðˌɪn ɐ skwˈɛɹ,mˈoʊst əv dʒˈɛnsənz lˈɛɾɚz ɑːɹ dɪzˈaɪnd wɪðˌɪn ɐ skwˈɛɹ, |0\n",
            "LJ001-0127.wav|ðə mˈɑːdɚn lˈɛɾɚz ɑːɹ nˈæɹoʊd baɪ ɐ θˈɜːd ɔːɹ ðɛɹɐbˈaʊt; bˌʌt wˌaɪl ðɪs ɡˈeɪn ʌv spˈeɪs vˈɛɹi mˈʌtʃ hˈæmpɚz ðə pˌɑːsəbˈɪlᵻɾi ʌv bjˈuːɾi ʌv dɪzˈaɪn,ðə mˈɑːdɚn lˈɛɾɚz ɑːɹ nˈæɹoʊd baɪ ɐ θˈɜːd ɔːɹ ðɛɹɐbˈaʊt; bˌʌt wˌaɪl ðɪs ɡˈeɪn ʌv spˈeɪs vˈɛɹi mˈʌtʃ hˈæmpɚz ðə pˌɑːsəbˈɪlᵻɾi ʌv bjˈuːɾi ʌv dɪzˈaɪn, |0\n",
            "LJ001-0128.wav|ɪɾ ɪz nˌɑːɾɚ ɹˈiːəl ɡˈeɪn, fɚðə mˈɑːdɚn pɹˈɪntɚ θɹˈoʊz ðə ɡˈeɪn ɐwˈeɪ baɪ pˈʊɾɪŋ ɪnˈɔːɹdᵻnətli wˈaɪd spˈeɪsᵻz bᵻtwˌiːn hɪz lˈaɪnz, wˈɪtʃ, pɹˈɑːbəbli,ɪɾ ɪz nˌɑːɾɚ ɹˈiːəl ɡˈeɪn, fɚðə mˈɑːdɚn pɹˈɪntɚ θɹˈoʊz ðə ɡˈeɪn ɐwˈeɪ baɪ pˈʊɾɪŋ ɪnˈɔːɹdᵻnətli wˈaɪd spˈeɪsᵻz bᵻtwˌiːn hɪz lˈaɪnz, wˈɪtʃ, pɹˈɑːbəbli, |0\n",
            "LJ001-0129.wav|ðə lˈæɾɚɹəl kəmpɹˈɛʃən ʌv hɪz lˈɛɾɚz ɹˈɛndɚz nˈɛsᵻsɚɹi.ðə lˈæɾɚɹəl kəmpɹˈɛʃən ʌv hɪz lˈɛɾɚz ɹˈɛndɚz nˈɛsᵻsɚɹi. |0\n",
            "LJ001-0130.wav|kəmˈɜːʃəlˌɪzəm ɐɡˈɛn kəmpˈɛlz ðə jˈuːs ʌv tˈaɪp tˈuː smˈɔːl ɪn sˈaɪz təbi kˈʌmftəbəl ɹˈiːdɪŋ:kəmˈɜːʃəlˌɪzəm ɐɡˈɛn kəmpˈɛlz ðə jˈuːs ʌv tˈaɪp tˈuː smˈɔːl ɪn sˈaɪz təbi kˈʌmftəbəl ɹˈiːdɪŋ: |0\n",
            "LJ001-0131.wav|ðə sˈaɪz nˈoʊn æz \"lˈɔŋ pɹˈaɪmɚ\" ˈɔːt təbi ðə smˈɔːlɪst sˈaɪz jˈuːzd ɪn ɐ bˈʊk mˈɛnt təbi ɹˈɛd.ðə sˈaɪz nˈoʊn æz \"lˈɔŋ pɹˈaɪmɚ\" ˈɔːt təbi ðə smˈɔːlɪst sˈaɪz jˈuːzd ɪn ɐ bˈʊk mˈɛnt təbi ɹˈɛd. |0\n",
            "LJ001-0132.wav|hˈɪɹ, ɐɡˈɛn, ɪf ðə pɹˈæktɪs ʌv \"lˈiːdɪŋ\" wɜː ɹᵻtɹˈɛntʃt lˈɑːɹdʒɚ tˈaɪp kʊd biː jˈuːzd wɪðˌaʊt ɛnhˈænsɪŋ ðə pɹˈaɪs əvə bˈʊk.hˈɪɹ, ɐɡˈɛn, ɪf ðə pɹˈæktɪs ʌv \"lˈiːdɪŋ\" wɜː ɹᵻtɹˈɛntʃt lˈɑːɹdʒɚ tˈaɪp kʊd biː jˈuːzd wɪðˌaʊt ɛnhˈænsɪŋ ðə pɹˈaɪs əvə bˈʊk. |0\n",
            "LJ001-0133.wav|wˈʌn vˈɛɹi ɪmpˈoːɹtənt mˈæɾɚɹ ˈɪn \"sˈɛɾɪŋ ˈʌp\" fɔːɹ fˈaɪn pɹˈɪntɪŋ ɪz ðə \"spˈeɪsɪŋ,\" ðæt ˈɪz, ðə lˈæɾɚɹəl dˈɪstəns ʌv wˈɜːdz fɹʌm wˈʌn ɐnˈʌðɚ.wˈʌn vˈɛɹi ɪmpˈoːɹtənt mˈæɾɚɹ ˈɪn \"sˈɛɾɪŋ ˈʌp\" fɔːɹ fˈaɪn pɹˈɪntɪŋ ɪz ðə \"spˈeɪsɪŋ,\" ðæt ˈɪz, ðə lˈæɾɚɹəl dˈɪstəns ʌv wˈɜːdz fɹʌm wˈʌn ɐnˈʌðɚ. |0\n",
            "LJ001-0134.wav|ɪn ɡˈʊd pɹˈɪntɪŋ ðə spˈeɪsᵻz bᵻtwˌiːn ðə wˈɜːdz ʃˌʊd biː æz nˌɪɹ æz pˈɑːsᵻbəl ˈiːkwəl ɪn ɡˈʊd pɹˈɪntɪŋ ðə spˈeɪsᵻz bᵻtwˌiːn ðə wˈɜːdz ʃˌʊd biː æz nˌɪɹ æz pˈɑːsᵻbəl ˈiːkwəl |0\n",
            "LJ001-0135.wav|ɪɾ ɪz ɪmpˈɑːsᵻbəl ðæt ðeɪ ʃˌʊd biː kwˈaɪt ˈiːkwəl ɛksˈɛpt ɪn lˈaɪnz ʌv pˈoʊɪtɹi ɪɾ ɪz ɪmpˈɑːsᵻbəl ðæt ðeɪ ʃˌʊd biː kwˈaɪt ˈiːkwəl ɛksˈɛpt ɪn lˈaɪnz ʌv pˈoʊɪtɹi |0\n",
            "LJ001-0136.wav|mˈɑːdɚn pɹˈɪntɚz ˌʌndɚstˈænd ðˈɪs, bˌʌt ɪɾ ɪz ˈoʊnli pɹˈæktɪst ɪnðə vˈɛɹi bˈɛst ɪstˈæblɪʃmənts.mˈɑːdɚn pɹˈɪntɚz ˌʌndɚstˈænd ðˈɪs, bˌʌt ɪɾ ɪz ˈoʊnli pɹˈæktɪst ɪnðə vˈɛɹi bˈɛst ɪstˈæblɪʃmənts. |0\n",
            "LJ001-0137.wav|bˌʌt ɐnˈʌðɚ pˈɔɪnt wˌɪtʃ ðeɪ ʃˌʊd ɐtˈɛnd tə ðeɪ ˈɔːlmoʊst ˈɔːlweɪz dˌɪsɹɪɡˈɑːɹd;bˌʌt ɐnˈʌðɚ pˈɔɪnt wˌɪtʃ ðeɪ ʃˌʊd ɐtˈɛnd tə ðeɪ ˈɔːlmoʊst ˈɔːlweɪz dˌɪsɹɪɡˈɑːɹd; |0\n",
            "LJ001-0138.wav|ðɪs ɪz ðə tˈɛndənsi tə ðə fɔːɹmˈeɪʃən ʌv ˈʌɡli miːˈændɚɹɪŋ wˈaɪt lˈaɪnz ɔːɹ \"ɹˈɪvɚz\" ɪnðə pˈeɪdʒ ðɪs ɪz ðə tˈɛndənsi tə ðə fɔːɹmˈeɪʃən ʌv ˈʌɡli miːˈændɚɹɪŋ wˈaɪt lˈaɪnz ɔːɹ \"ɹˈɪvɚz\" ɪnðə pˈeɪdʒ |0\n",
            "LJ001-0139.wav|ɐ blˈɛmɪʃ wˌɪtʃ kæn biː nˈɪɹli, ðˌoʊ nˌɑːt hˈoʊli, ɐvˈɔɪdᵻd baɪ kˈɛɹ ænd fˈɔːɹθɔːt ɐ blˈɛmɪʃ wˌɪtʃ kæn biː nˈɪɹli, ðˌoʊ nˌɑːt hˈoʊli, ɐvˈɔɪdᵻd baɪ kˈɛɹ ænd fˈɔːɹθɔːt |0\n",
            "LJ001-0140.wav|ðə dɪzˈaɪɚɹəbəl θˈɪŋ bˈiːɪŋ \"ðə bɹˈeɪkɪŋ ʌvðə lˈaɪn\" æz ɪn bˈɑːndɪŋ mˈeɪsənɹi ɔːɹ bɹˈɪkwɜːk ðə dɪzˈaɪɚɹəbəl θˈɪŋ bˈiːɪŋ \"ðə bɹˈeɪkɪŋ ʌvðə lˈaɪn\" æz ɪn bˈɑːndɪŋ mˈeɪsənɹi ɔːɹ bɹˈɪkwɜːk |0\n",
            "LJ001-0141.wav|ðə dʒˈɛnɚɹəl səlˈɪdᵻɾi əvə pˈeɪdʒ ɪz mˈʌtʃ təbi sˈɔːt fɔːɹ ðə dʒˈɛnɚɹəl səlˈɪdᵻɾi əvə pˈeɪdʒ ɪz mˈʌtʃ təbi sˈɔːt fɔːɹ |0\n",
            "LJ001-0142.wav|mˈɑːdɚn pɹˈɪntɚz dʒˈɛnɚɹəli ˌoʊvɚdˈuː ðə \"wˈaɪts\" ɪnðə spˈeɪsɪŋ, ɐ dˈiːfɛkt pɹˈɑːbəbli fˈoːɹst ˌɔn ðˌɛm baɪ ðə kˈæɹɪktɚləs kwˈɔlᵻɾi ʌvðə lˈɛɾɚz.mˈɑːdɚn pɹˈɪntɚz dʒˈɛnɚɹəli ˌoʊvɚdˈuː ðə \"wˈaɪts\" ɪnðə spˈeɪsɪŋ, ɐ dˈiːfɛkt pɹˈɑːbəbli fˈoːɹst ˌɔn ðˌɛm baɪ ðə kˈæɹɪktɚləs kwˈɔlᵻɾi ʌvðə lˈɛɾɚz. |0\n",
            "LJ001-0143.wav|fɔːɹ wˌɛɹ ðiːz ɑːɹ bˈoʊldli ænd kˈɛɹfəli dɪzˈaɪnd, ænd ˈiːtʃ lˈɛɾɚɹ ɪz θˈʌɹoʊli ˌɪndᵻvˈɪdʒuːəl ɪn fˈɔːɹm,fɔːɹ wˌɛɹ ðiːz ɑːɹ bˈoʊldli ænd kˈɛɹfəli dɪzˈaɪnd, ænd ˈiːtʃ lˈɛɾɚɹ ɪz θˈʌɹoʊli ˌɪndᵻvˈɪdʒuːəl ɪn fˈɔːɹm, |0\n",
            "LJ001-0144.wav|ðə wˈɜːdz mˈeɪ biː sˈɛt mˈʌtʃ klˈoʊsɚ təɡˈɛðɚ, wɪðˌaʊt lˈɔs ʌv klˈɪɹnəs.ðə wˈɜːdz mˈeɪ biː sˈɛt mˈʌtʃ klˈoʊsɚ təɡˈɛðɚ, wɪðˌaʊt lˈɔs ʌv klˈɪɹnəs. |0\n",
            "LJ001-0145.wav|nˈoʊ dˈɛfɪnət ɹˈuːlz, haʊˈɛvɚ, ɛksˈɛpt ðɪ ɐvˈɔɪdəns ʌv \"ɹˈɪvɚz\" ænd ɛksˈɛs ʌv wˈaɪt, kæn biː ɡˈɪvən fɚðə spˈeɪsɪŋ,nˈoʊ dˈɛfɪnət ɹˈuːlz, haʊˈɛvɚ, ɛksˈɛpt ðɪ ɐvˈɔɪdəns ʌv \"ɹˈɪvɚz\" ænd ɛksˈɛs ʌv wˈaɪt, kæn biː ɡˈɪvən fɚðə spˈeɪsɪŋ, |0\n",
            "LJ001-0146.wav|wˌɪtʃ ɹᵻkwˈaɪɚz ðə kˈɑːnstənt ˈɛksɚsˌaɪz ʌv dʒˈʌdʒmənt ænd tˈeɪst ɔnðə pˈɑːɹt ʌvðə pɹˈɪntɚ.wˌɪtʃ ɹᵻkwˈaɪɚz ðə kˈɑːnstənt ˈɛksɚsˌaɪz ʌv dʒˈʌdʒmənt ænd tˈeɪst ɔnðə pˈɑːɹt ʌvðə pɹˈɪntɚ. |0\n",
            "LJ001-0147.wav|ðə pəzˈɪʃən ʌvðə pˈeɪdʒ ɔnðə pˈeɪpɚ ʃˌʊd biː kənsˈɪdɚd ɪf ðə bˈʊk ɪz tə hæv ɐ sˌæɾɪsfˈæktɚɹi lˈʊk.ðə pəzˈɪʃən ʌvðə pˈeɪdʒ ɔnðə pˈeɪpɚ ʃˌʊd biː kənsˈɪdɚd ɪf ðə bˈʊk ɪz tə hæv ɐ sˌæɾɪsfˈæktɚɹi lˈʊk. |0\n",
            "LJ001-0148.wav|hˈɪɹ wˈʌns mˈoːɹ ðɪ ˈɔːlmoʊst ɪnvˈɛɹɪəbəl mˈɑːdɚn pɹˈæktɪs ɪz ɪn ˌɑːpəzˈɪʃən tʊ ɐ nˈætʃɚɹəl sˈɛns ʌv pɹəpˈoːɹʃən.hˈɪɹ wˈʌns mˈoːɹ ðɪ ˈɔːlmoʊst ɪnvˈɛɹɪəbəl mˈɑːdɚn pɹˈæktɪs ɪz ɪn ˌɑːpəzˈɪʃən tʊ ɐ nˈætʃɚɹəl sˈɛns ʌv pɹəpˈoːɹʃən. |0\n",
            "LJ001-0149.wav|fɹʌmðə tˈaɪm wɛn bˈʊks fˈɜːst tˈʊk ðɛɹ pɹˈɛzənt ʃˈeɪp tˈɪl ðɪ ˈɛnd ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹi, ɔːɹ ˌɪndˈiːd lˈeɪɾɚ,fɹʌmðə tˈaɪm wɛn bˈʊks fˈɜːst tˈʊk ðɛɹ pɹˈɛzənt ʃˈeɪp tˈɪl ðɪ ˈɛnd ʌvðə sˈɪkstiːnθ sˈɛntʃɚɹi, ɔːɹ ˌɪndˈiːd lˈeɪɾɚ, |0\n",
            "LJ001-0150.wav|ðə pˈeɪdʒ sˌoʊ lˈeɪ ɔnðə pˈeɪpɚ ðæt ðɛɹwˌʌz mˈoːɹ spˈeɪs ɐlˈaʊd tə ðə bˈɑːɾəm ænd fˈɔːɹ mˈɑːɹdʒɪn ðɐn tə ðə tˈɑːp ænd bˈæk ʌvðə pˈeɪpɚ,ðə pˈeɪdʒ sˌoʊ lˈeɪ ɔnðə pˈeɪpɚ ðæt ðɛɹwˌʌz mˈoːɹ spˈeɪs ɐlˈaʊd tə ðə bˈɑːɾəm ænd fˈɔːɹ mˈɑːɹdʒɪn ðɐn tə ðə tˈɑːp ænd bˈæk ʌvðə pˈeɪpɚ, |0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config['data_params']['root_path'] = \"Data/\"\n",
        "\n",
        "config['batch_size'] = 2 # not enough RAM\n",
        "config['save_freq'] = 1\n",
        "config['max_len'] = 100 # not enough RAM\n",
        "config['epochs'] = 3\n",
        "config['loss_params']['diff_epoch'] = 2\n",
        "config['loss_params']['joint_epoch'] = 110 # we do not do SLM adversarial training due to not enough RAM\n",
        "\n",
        "with open(config_path, 'w') as outfile:\n",
        "  yaml.dump(config, outfile, default_flow_style=True)"
      ],
      "metadata": {
        "id": "TPTRgOKSVT4K"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start finetuning\n"
      ],
      "metadata": {
        "id": "uUuB_19NWj2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_finetune.py --config_path ./Configs/config_ft.yml"
      ],
      "metadata": {
        "id": "HZVAD5GKWm-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557140a2-3b16-4dd0-dfae-d63c7db513df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-03 17:28:24.533552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1741022904.554919    8808 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1741022904.561527    8808 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "bert loaded\n",
            "bert_encoder loaded\n",
            "predictor loaded\n",
            "decoder loaded\n",
            "text_encoder loaded\n",
            "predictor_encoder loaded\n",
            "style_encoder loaded\n",
            "diffusion loaded\n",
            "text_aligner loaded\n",
            "pitch_extractor loaded\n",
            "mpd loaded\n",
            "msd loaded\n",
            "wd loaded\n",
            "BERT AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    base_momentum: 0.85\n",
            "    betas: (0.9, 0.99)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-09\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 1e-05\n",
            "    lr: 1e-05\n",
            "    max_lr: 2e-05\n",
            "    max_momentum: 0.95\n",
            "    maximize: False\n",
            "    min_lr: 0\n",
            "    weight_decay: 0.01\n",
            ")\n",
            "decoder AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    base_momentum: 0.85\n",
            "    betas: (0.0, 0.99)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-09\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    initial_lr: 0.0001\n",
            "    lr: 0.0001\n",
            "    max_lr: 0.0002\n",
            "    max_momentum: 0.95\n",
            "    maximize: False\n",
            "    min_lr: 0\n",
            "    weight_decay: 0.0001\n",
            ")\n",
            "bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti tˈuː)bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ fˈoːɹtiːn sˈɪkstitˈuː) \n",
            "bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti tˈuː)bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ fˈoːɹtiːn sˈɪkstitˈuː) \n",
            "bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti tˈuː)bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ fˈoːɹtiːn sˈɪkstitˈuː) \n",
            "bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ wˈʌn θˈaʊzənd fˈoːɹhˈʌndɹɪd sˈɪksti tˈuː)bˌʌt ðə fˈɜːst bˈaɪbəl ˈæktʃuːəli dˈeɪɾᵻd (wˌɪtʃ ˈɔːlsoʊ wʌz pɹˈɪntᵻd æt mˈeɪnts baɪ pˈiːɾɚ skˈoʊfɚɹ ɪnðə jˈɪɹ fˈoːɹtiːn sˈɪkstitˈuː) \n",
            "ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn wˈʌn θˈaʊzənd sˈɛvənhˈʌndɹɪd twˈɛnti)ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn sˈɛvəntˌiːn twˈɛnti) \n",
            "ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn wˈʌn θˈaʊzənd sˈɛvənhˈʌndɹɪd twˈɛnti)ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn sˈɛvəntˌiːn twˈɛnti) \n",
            "ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn wˈʌn θˈaʊzənd sˈɛvənhˈʌndɹɪd twˈɛnti)ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn sˈɛvəntˌiːn twˈɛnti) \n",
            "ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn wˈʌn θˈaʊzənd sˈɛvənhˈʌndɹɪd twˈɛnti)ɪn ˈɪŋɡlənd ɐbˌaʊt ðɪs tˈaɪm, ɐn ɐtˈɛmpt wʌz mˈeɪd (nˈoʊɾəbli baɪ kˈæslɑːn, hˌuː stˈɑːɹɾᵻd bˈɪznəs ɪn lˈʌndən æz ɐ tˈaɪpfˈaʊndɚɹ ɪn sˈɛvəntˌiːn twˈɛnti) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model quality\n",
        "\n",
        "Note that this mainly serves as a proof of concept due to RAM limitation of free Colab instances. A lot of settings are suboptimal. In the future when DDP works for train_second.py, we will also add mixed precision finetuning to save time and RAM. You can also add SLM adversarial training run if you have paid Colab services (such as A100 with 40G of RAM)."
      ],
      "metadata": {
        "id": "I0_7wsGkXGfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "OPLphjbncE7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabc2ad6-9d16-4c07-a91b-d8580b832e84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# load packages\n",
        "import time\n",
        "import random\n",
        "import yaml\n",
        "from munch import Munch\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import librosa\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from models import *\n",
        "from utils import *\n",
        "from text_utils import TextCleaner\n",
        "textclenaer = TextCleaner()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "to_mel = torchaudio.transforms.MelSpectrogram(\n",
        "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
        "mean, std = -4, 4\n",
        "\n",
        "def length_to_mask(lengths):\n",
        "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
        "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
        "    return mask\n",
        "\n",
        "def preprocess(wave):\n",
        "    wave_tensor = torch.from_numpy(wave).float()\n",
        "    mel_tensor = to_mel(wave_tensor)\n",
        "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
        "    return mel_tensor\n",
        "\n",
        "def compute_style(path):\n",
        "    wave, sr = librosa.load(path, sr=24000)\n",
        "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
        "    if sr != 24000:\n",
        "        audio = librosa.resample(audio, sr, 24000)\n",
        "    mel_tensor = preprocess(audio).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
        "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
        "\n",
        "    return torch.cat([ref_s, ref_p], dim=1)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# load phonemizer\n",
        "import phonemizer\n",
        "global_phonemizer = phonemizer.backend.EspeakBackend(language='en-us', preserve_punctuation=True,  with_stress=True)\n",
        "\n",
        "config = yaml.safe_load(open(\"Models/LJSpeech/config_ft.yml\"))\n",
        "\n",
        "# load pretrained ASR model\n",
        "ASR_config = config.get('ASR_config', False)\n",
        "ASR_path = config.get('ASR_path', False)\n",
        "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
        "\n",
        "# load pretrained F0 model\n",
        "F0_path = config.get('F0_path', False)\n",
        "pitch_extractor = load_F0_models(F0_path)\n",
        "\n",
        "# load BERT model\n",
        "from Utils.PLBERT.util import load_plbert\n",
        "BERT_path = config.get('PLBERT_dir', False)\n",
        "plbert = load_plbert(BERT_path)\n",
        "\n",
        "model_params = recursive_munch(config['model_params'])\n",
        "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
        "_ = [model[key].eval() for key in model]\n",
        "_ = [model[key].to(device) for key in model]"
      ],
      "metadata": {
        "id": "jIIAoDACXJL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f12ea79-f549-49e1-aee9-175ffd147073"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/StyleTTS2/models.py:604: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  params = torch.load(model_path, map_location='cpu')['model']\n",
            "/content/StyleTTS2/models.py:588: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  params = torch.load(path, map_location='cpu')['net']\n",
            "/content/StyleTTS2/Utils/PLBERT/util.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location='cpu')\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f for f in os.listdir(\"Models/LJSpeech/\") if f.endswith('.pth')]\n",
        "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))"
      ],
      "metadata": {
        "id": "eKXRAyyzcMpQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_whole = torch.load(\"Models/LJSpeech/\" + sorted_files[-1], map_location='cpu')\n",
        "params = params_whole['net']"
      ],
      "metadata": {
        "id": "ULuU9-VDb9Pk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5251e703-5282-445f-e219-a93fbb6d8c23"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-febab44deea8>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  params_whole = torch.load(\"Models/LJSpeech/\" + sorted_files[-1], map_location='cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in model:\n",
        "    if key in params:\n",
        "        print('%s loaded' % key)\n",
        "        try:\n",
        "            model[key].load_state_dict(params[key])\n",
        "        except:\n",
        "            from collections import OrderedDict\n",
        "            state_dict = params[key]\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:] # remove `module.`\n",
        "                new_state_dict[name] = v\n",
        "            # load params\n",
        "            model[key].load_state_dict(new_state_dict, strict=False)\n",
        "#             except:\n",
        "#                 _load(params[key], model[key])\n",
        "_ = [model[key].eval() for key in model]"
      ],
      "metadata": {
        "id": "J-U29yIYc2ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8c0713-9a2c-4525-9414-df114d719000"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert loaded\n",
            "bert_encoder loaded\n",
            "predictor loaded\n",
            "decoder loaded\n",
            "text_encoder loaded\n",
            "predictor_encoder loaded\n",
            "style_encoder loaded\n",
            "diffusion loaded\n",
            "text_aligner loaded\n",
            "pitch_extractor loaded\n",
            "mpd loaded\n",
            "msd loaded\n",
            "wd loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule"
      ],
      "metadata": {
        "id": "jrPQ_Yrwc3n6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = DiffusionSampler(\n",
        "    model.diffusion.diffusion,\n",
        "    sampler=ADPM2Sampler(),\n",
        "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
        "    clamp=False\n",
        ")"
      ],
      "metadata": {
        "id": "n2CWYNoqc455"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, ref_s, alpha = 0.3, beta = 0.7, diffusion_steps=5, embedding_scale=1):\n",
        "    text = text.strip()\n",
        "    ps = global_phonemizer.phonemize([text])\n",
        "    #ps = word_tokenize(ps[0])\n",
        "    ps = ' '.join(ps)\n",
        "    tokens = textclenaer(ps)\n",
        "    tokens.insert(0, 0)\n",
        "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
        "        text_mask = length_to_mask(input_lengths).to(device)\n",
        "\n",
        "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
        "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
        "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
        "\n",
        "        s_pred = sampler(noise = torch.randn((1, 256)).unsqueeze(1).to(device),\n",
        "                                          embedding=bert_dur,\n",
        "                                          embedding_scale=embedding_scale,\n",
        "                                            features=ref_s, # reference from the same speaker as the embedding\n",
        "                                             num_steps=diffusion_steps).squeeze(1)\n",
        "\n",
        "\n",
        "        s = s_pred[:, 128:]\n",
        "        ref = s_pred[:, :128]\n",
        "\n",
        "        ref = alpha * ref + (1 - alpha)  * ref_s[:, :128]\n",
        "        s = beta * s + (1 - beta)  * ref_s[:, 128:]\n",
        "\n",
        "        d = model.predictor.text_encoder(d_en,\n",
        "                                         s, input_lengths, text_mask)\n",
        "\n",
        "        x, _ = model.predictor.lstm(d)\n",
        "        duration = model.predictor.duration_proj(x)\n",
        "\n",
        "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
        "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
        "\n",
        "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
        "        c_frame = 0\n",
        "        for i in range(pred_aln_trg.size(0)):\n",
        "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
        "            c_frame += int(pred_dur[i].data)\n",
        "\n",
        "        # encode prosody\n",
        "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        if model_params.decoder.type == \"hifigan\":\n",
        "            asr_new = torch.zeros_like(en)\n",
        "            asr_new[:, :, 0] = en[:, :, 0]\n",
        "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
        "            en = asr_new\n",
        "\n",
        "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
        "\n",
        "        asr = (t_en @ pred_aln_trg.unsqueeze(0).to(device))\n",
        "        if model_params.decoder.type == \"hifigan\":\n",
        "            asr_new = torch.zeros_like(asr)\n",
        "            asr_new[:, :, 0] = asr[:, :, 0]\n",
        "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
        "            asr = asr_new\n",
        "\n",
        "        out = model.decoder(asr,\n",
        "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
        "\n",
        "\n",
        "    return out.squeeze().cpu().numpy()[..., :-50] # weird pulse at the end of the model, need to be fixed later"
      ],
      "metadata": {
        "id": "2x5kVb3nc_eY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synthesize speech"
      ],
      "metadata": {
        "id": "O159JnwCc6CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''Maltby and Company would issue warrants on them deliverable to the importer, and the goods were then passed to be stored in neighboring warehouses.\n",
        "'''"
      ],
      "metadata": {
        "id": "ThciXQ6rc9Eq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a random reference in the training set, note that it doesn't matter which one you use\n",
        "path = \"Data/LJ001-0110.wav\"\n",
        "# this style vector ref_s can be saved as a parameter together with the model weights\n",
        "ref_s = compute_style(path)"
      ],
      "metadata": {
        "id": "jldPkJyCc83a"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "wav = inference(text, ref_s, alpha=0.9, beta=0.9, diffusion_steps=10, embedding_scale=1)\n",
        "rtf = (time.time() - start) / (len(wav) / 24000)\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "import IPython.display as ipd\n",
        "display(ipd.Audio(wav, rate=24000, normalize=False))"
      ],
      "metadata": {
        "id": "_mIU0jqDdQ-c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "2b1a598b-31e0-4e5d-d211-dd52bb972abd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-70c8d52aa18a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffusion_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m24000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"RTF = {rtf:5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b7a6852c285b>\u001b[0m in \u001b[0;36minference\u001b[0;34m(text, ref_s, alpha, beta, diffusion_steps, embedding_scale)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpred_dur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mpred_aln_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mc_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_aln_trg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
          ]
        }
      ]
    }
  ]
}